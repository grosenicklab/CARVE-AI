{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547ea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebbff689",
   "metadata": {},
   "source": [
    "#### DEFINE DATA LOADER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76de7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE DATA LOADER FUNCTIONS ####\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "class DataGeneratorPPCA(Dataset):\n",
    "\n",
    "    def __init__(self, dims, hdims, min_sv=0.11, max_sv=5.0, sigma_sq=0.1, deterministic=True, total=10000):\n",
    "        self.dims = dims\n",
    "        self.hdims = hdims\n",
    "\n",
    "        self.eigs = min_sv + (max_sv - min_sv) * np.linspace(0, 1, hdims)\n",
    "        self.eigvectors = ortho_group.rvs(dims)[:, :hdims]\n",
    "        self.w = np.matmul(self.eigvectors, np.diag(np.sqrt(self.eigs - sigma_sq)))\n",
    "\n",
    "        self.sigma_sq = sigma_sq\n",
    "        self.sigma = np.sqrt(sigma_sq)\n",
    "\n",
    "        self.total = total\n",
    "        self.deterministic = deterministic\n",
    "        if self.deterministic:\n",
    "            self.z_sample = np.random.normal(size=(total, self.hdims))\n",
    "            self.x_sample = np.random.normal(np.matmul(self.z_sample, self.w.T), self.sigma).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.deterministic:\n",
    "            return self.x_sample[i]\n",
    "        else:\n",
    "            z_sample = np.random.normal(size=self.hdims)\n",
    "            return np.random.normal(self.w.dot(z_sample), self.sigma).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return a large number for an epoch\n",
    "        return self.total\n",
    "\n",
    "\n",
    "class DataGeneratorPCA(Dataset):\n",
    "    def __init__(self, dims, hdims, min_sv=0.11, max_sv=5.0, total=10000, sv_list=None,\n",
    "                 load_data=None):\n",
    "        self.dims = dims\n",
    "        self.hdims = hdims\n",
    "\n",
    "        if load_data is None:\n",
    "            if isinstance(sv_list, list):\n",
    "                assert len(sv_list) == dims\n",
    "                self.full_eigs = np.array(sorted(sv_list, reverse=True))\n",
    "            else:\n",
    "                self.full_eigs = min_sv + (max_sv - min_sv) * np.linspace(1, 0, dims)\n",
    "            self.eigs = self.full_eigs[:hdims]\n",
    "\n",
    "            self.full_svs = np.sqrt(self.full_eigs)\n",
    "\n",
    "            self.full_eigvectors = ortho_group.rvs(dims)\n",
    "            self.eigvectors = self.full_eigvectors[:, :hdims]\n",
    "\n",
    "            self.total = total\n",
    "\n",
    "            self.full_z_sample = np.random.normal(size=(total, self.dims))\n",
    "            self.x_sample = (self.full_eigvectors @ np.diag(self.full_svs) @ self.full_z_sample.T).T.astype(np.float32)\n",
    "\n",
    "        else:\n",
    "            self.x_sample = load_data\n",
    "            u, s, vh = np.linalg.svd(self.x_sample.T, full_matrices=False)\n",
    "            self.eigs = s[:self.hdims]\n",
    "            self.eigvectors = u[:, :self.hdims]\n",
    "            self.total = len(self.x_sample)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x_sample[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.x_sample.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9beac",
   "metadata": {},
   "source": [
    "#### DEFINE MODEL CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c927a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE MODEL CLASSES ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self, model_name, model_type, model_class, input_dim, hidden_dim, init_scale, optim_class, lr,\n",
    "                 extra_model_args={}, extra_optim_args={}):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model_class = model_class\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_scale = init_scale\n",
    "        self.extra_model_args = extra_model_args\n",
    "\n",
    "        self.optim_class = optim_class\n",
    "        self.lr = lr\n",
    "        self.extra_optim_args = extra_optim_args\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model_class(input_dim=input_dim, hidden_dim=hidden_dim, init_scale=init_scale, **extra_model_args).to(device)\n",
    "\n",
    "        self.optimizer = optim_class(self.model.parameters(), lr=lr, **extra_optim_args)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self.model_type\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "class LinearAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, hidden_dim, init_scale=0.001,\n",
    "                 weight_reg_type=None, l2_reg_list=None):\n",
    "        super(LinearAE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "\n",
    "        self.weight_reg_type = weight_reg_type\n",
    "        self.l2_reg_scalar = None\n",
    "        self.l2_reg_list = l2_reg_list\n",
    "\n",
    "        self.encoder.weight.data.normal_(0.0, init_scale)\n",
    "        self.decoder.weight.data.normal_(0.0, init_scale)\n",
    "\n",
    "        # configure regularization parameters\n",
    "\n",
    "        assert self.weight_reg_type is None or isinstance(self.l2_reg_list, list), \\\n",
    "            \"l2_reg_list must be a list if weight_reg_type is not None\"\n",
    "\n",
    "        assert self.l2_reg_list is None or len(self.l2_reg_list) == hidden_dim, \\\n",
    "            \"Length of l2_reg_list must match latent dimension\"\n",
    "\n",
    "        if weight_reg_type in (\"uniform_product\", \"uniform_sum\"):\n",
    "            self.l2_reg_scalar = l2_reg_list[0] ** 2    # more efficient to use scalar than diag_weights\n",
    "\n",
    "        elif weight_reg_type == \"non_uniform_sum\":\n",
    "            self.reg_weights = torch.tensor(\n",
    "                np.array(self.l2_reg_list).astype(np.float32)\n",
    "            )\n",
    "            self.diag_weights = nn.Parameter(torch.diag(self.reg_weights), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.get_reconstruction_loss(x) + self._get_reg_loss()\n",
    "\n",
    "    def compute_trace_norm(self):\n",
    "        \"\"\"\n",
    "        Computes the trace norm of the autoencoder, as well as decoder and encoder individually\n",
    "        :return: trace_norm(W2W1), trace_norm(W1), trace_norm(W2)\n",
    "        \"\"\"\n",
    "        return torch.matmul(self.decoder.weight, self.encoder.weight).norm(p='nuc'), \\\n",
    "               self.encoder.weight.norm(p='nuc'), \\\n",
    "               self.decoder.weight.norm(p='nuc'),\n",
    "\n",
    "    def get_reconstruction_loss(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "\n",
    "        recon_loss = torch.sum((x - recon) ** 2) / len(x)\n",
    "        return recon_loss\n",
    "\n",
    "    def get_reg_weights_np(self):\n",
    "        if self.weight_reg_type is None:\n",
    "            return np.zeros(self.hidden_dim)\n",
    "        return np.array(self.l2_reg_list)\n",
    "\n",
    "    def _get_reg_loss(self):\n",
    "        # Standard L2 regularization, applied to W2W1 (product loss)\n",
    "        if self.weight_reg_type == 'uniform_product':\n",
    "            return self.l2_reg_scalar * (torch.norm(torch.matmul(self.decoder.weight, self.encoder.weight)) ** 2)\n",
    "\n",
    "        # Standard L2 regularization for encoder and decoder separately (sum loss)\n",
    "        elif self.weight_reg_type == 'uniform_sum':\n",
    "            # regularize both encoder and decoder\n",
    "            return self.l2_reg_scalar * (torch.norm(self.encoder.weight) ** 2 + torch.norm(self.decoder.weight) ** 2)\n",
    "\n",
    "        # non-uniform sum\n",
    "        elif self.weight_reg_type == 'non_uniform_sum':\n",
    "            return torch.norm(self.diag_weights @ self.encoder.weight) ** 2 \\\n",
    "                   + torch.norm(self.decoder.weight @ self.diag_weights) ** 2\n",
    "\n",
    "        # Do not apply regularization\n",
    "        elif self.weight_reg_type is None:\n",
    "            return 0.0\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"weight_reg_type should be one of (uniform_product, uniform_sum, non_uniform_sum, None)\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3428f3",
   "metadata": {},
   "source": [
    "#### DEFINE MODEL TRAINING FUNCTION train_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4c622b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE MODEL TRAINING FUNCTION train_models ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_models(data_loader, train_itr, metrics_dict, model_configs, eval_metrics_list=None):\n",
    "    # Initialize model\n",
    "\n",
    "    for train_i in range(train_itr):\n",
    "        for x in data_loader:\n",
    "            x_cuda = x.to(device)\n",
    "\n",
    "            # ---- Optimize ----\n",
    "            losses = {}\n",
    "#             for model_config in model_configs:\n",
    "            model = model_config.get_model()\n",
    "            optimizer = model_config.get_optimizer()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(x_cuda)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # ROTATION\n",
    "            y = model.encoder.weight @ x_cuda.T\n",
    "            yy_t_norm = y @ y.T / float(len(x))\n",
    "            yy_t_upper = yy_t_norm - yy_t_norm.tril()\n",
    "            gamma = 0.5 * (yy_t_upper - yy_t_upper.T)\n",
    "            model.encoder.weight.grad -= gamma @ model.encoder.weight\n",
    "            model.decoder.weight.grad -= model.decoder.weight @ gamma.T\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[model_config.name] = loss.item()\n",
    "\n",
    "        # ---- Log statistics ----\n",
    "        if train_i == 0 or (train_i + 1) % 10 == 0:\n",
    "            print(\"\".join([\"Iteration = {}, Losses: \".format(train_i + 1)]\n",
    "                          + [\"{} = {} \".format(key, val) for key, val in losses.items()]))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45282d5",
   "metadata": {},
   "source": [
    "### TRAIN A MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef555b7d",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b532dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET DATA ####\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed=1\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_dim = 1000\n",
    "hidden_dim = 5\n",
    "\n",
    "n_data = 5000\n",
    "batch_size = n_data\n",
    "\n",
    "max_sv = float(input_dim) * 0.1\n",
    "min_sv = 1.0\n",
    "sigma = 0.5\n",
    "\n",
    "gt_data = DataGeneratorPCA(input_dim, hidden_dim, min_sv=min_sv, max_sv=max_sv, total=n_data)\n",
    "data = DataGeneratorPCA(input_dim, hidden_dim, load_data=gt_data.x_sample)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdabf03",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad72211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the model ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### DEFINE MODEL #####\n",
    "model_dict = dict(\n",
    "    model_name='rotation',\n",
    "    model_type='rotation',\n",
    "    model_class=LinearAE,\n",
    "    extra_model_args = {\"weight_reg_type\": None},\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    init_scale=0.0001,\n",
    "    optim_class=torch.optim.Adam,\n",
    "    lr=0.0003,\n",
    "    extra_optim_args={},\n",
    "    train_itr=50000,\n",
    "    seed=seed\n",
    ")\n",
    "#     extra_model_args=torch.optim.SGD\n",
    "#     optim_class={'momentum': 0.9, 'nesterov': True},\n",
    "\n",
    "# model config contains the model \n",
    "model_config = ModelConfig(\n",
    "        model_name=model_dict['model_name'],\n",
    "        model_type=model_dict['model_type'],\n",
    "        model_class=model_dict['model_class'],\n",
    "        input_dim=model_dict['input_dim'], \n",
    "        hidden_dim=model_dict['hidden_dim'],\n",
    "        init_scale=model_dict['init_scale'],\n",
    "        extra_model_args=model_dict['extra_model_args'],\n",
    "        optim_class=model_dict['optim_class'],\n",
    "        lr=model_dict['lr'],\n",
    "        extra_optim_args=model_dict['extra_optim_args']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1, Losses: rotation = 50466.04296875 \n",
      "Iteration = 10, Losses: rotation = 50461.56640625 \n",
      "Iteration = 20, Losses: rotation = 50439.94921875 \n",
      "Iteration = 30, Losses: rotation = 50394.66796875 \n",
      "Iteration = 40, Losses: rotation = 50324.515625 \n",
      "Iteration = 50, Losses: rotation = 50235.48046875 \n",
      "Iteration = 60, Losses: rotation = 50140.59375 \n",
      "Iteration = 70, Losses: rotation = 50055.4609375 \n",
      "Iteration = 80, Losses: rotation = 49990.68359375 \n",
      "Iteration = 90, Losses: rotation = 49947.28125 \n",
      "Iteration = 100, Losses: rotation = 49919.53125 \n",
      "Iteration = 110, Losses: rotation = 49900.75390625 \n",
      "Iteration = 120, Losses: rotation = 49886.57421875 \n",
      "Iteration = 130, Losses: rotation = 49874.92578125 \n",
      "Iteration = 140, Losses: rotation = 49864.92578125 \n",
      "Iteration = 150, Losses: rotation = 49856.15234375 \n",
      "Iteration = 160, Losses: rotation = 49848.33203125 \n",
      "Iteration = 170, Losses: rotation = 49841.32421875 \n",
      "Iteration = 180, Losses: rotation = 49835.0 \n",
      "Iteration = 190, Losses: rotation = 49829.28125 \n",
      "Iteration = 200, Losses: rotation = 49824.109375 \n",
      "Iteration = 210, Losses: rotation = 49819.40234375 \n",
      "Iteration = 220, Losses: rotation = 49815.125 \n",
      "Iteration = 230, Losses: rotation = 49811.234375 \n",
      "Iteration = 240, Losses: rotation = 49807.69140625 \n",
      "Iteration = 250, Losses: rotation = 49804.453125 \n",
      "Iteration = 260, Losses: rotation = 49801.49609375 \n",
      "Iteration = 270, Losses: rotation = 49798.78515625 \n",
      "Iteration = 280, Losses: rotation = 49796.296875 \n",
      "Iteration = 290, Losses: rotation = 49794.0078125 \n",
      "Iteration = 300, Losses: rotation = 49791.90625 \n",
      "Iteration = 310, Losses: rotation = 49789.96484375 \n",
      "Iteration = 320, Losses: rotation = 49788.16796875 \n",
      "Iteration = 330, Losses: rotation = 49786.4921875 \n",
      "Iteration = 340, Losses: rotation = 49784.94140625 \n",
      "Iteration = 350, Losses: rotation = 49783.49609375 \n",
      "Iteration = 360, Losses: rotation = 49782.14453125 \n",
      "Iteration = 370, Losses: rotation = 49780.875 \n",
      "Iteration = 380, Losses: rotation = 49779.69140625 \n",
      "Iteration = 390, Losses: rotation = 49778.5703125 \n",
      "Iteration = 400, Losses: rotation = 49777.515625 \n",
      "Iteration = 410, Losses: rotation = 49776.515625 \n",
      "Iteration = 420, Losses: rotation = 49775.56640625 \n",
      "Iteration = 430, Losses: rotation = 49774.6640625 \n",
      "Iteration = 440, Losses: rotation = 49773.8046875 \n",
      "Iteration = 450, Losses: rotation = 49772.98046875 \n",
      "Iteration = 460, Losses: rotation = 49772.203125 \n",
      "Iteration = 470, Losses: rotation = 49771.44921875 \n",
      "Iteration = 480, Losses: rotation = 49770.7265625 \n",
      "Iteration = 490, Losses: rotation = 49770.03125 \n",
      "Iteration = 500, Losses: rotation = 49769.37109375 \n",
      "Iteration = 510, Losses: rotation = 49768.7265625 \n",
      "Iteration = 520, Losses: rotation = 49768.10546875 \n",
      "Iteration = 530, Losses: rotation = 49767.5078125 \n",
      "Iteration = 540, Losses: rotation = 49766.92578125 \n",
      "Iteration = 550, Losses: rotation = 49766.36328125 \n",
      "Iteration = 560, Losses: rotation = 49765.8203125 \n",
      "Iteration = 570, Losses: rotation = 49765.30078125 \n",
      "Iteration = 580, Losses: rotation = 49764.7890625 \n",
      "Iteration = 590, Losses: rotation = 49764.29296875 \n",
      "Iteration = 600, Losses: rotation = 49763.81640625 \n",
      "Iteration = 610, Losses: rotation = 49763.3515625 \n",
      "Iteration = 620, Losses: rotation = 49762.8984375 \n",
      "Iteration = 630, Losses: rotation = 49762.46484375 \n",
      "Iteration = 640, Losses: rotation = 49762.0390625 \n",
      "Iteration = 650, Losses: rotation = 49761.62890625 \n",
      "Iteration = 660, Losses: rotation = 49761.23046875 \n",
      "Iteration = 670, Losses: rotation = 49760.84375 \n",
      "Iteration = 680, Losses: rotation = 49760.46875 \n",
      "Iteration = 690, Losses: rotation = 49760.11328125 \n",
      "Iteration = 700, Losses: rotation = 49759.76171875 \n",
      "Iteration = 710, Losses: rotation = 49759.42578125 \n",
      "Iteration = 720, Losses: rotation = 49759.1015625 \n",
      "Iteration = 730, Losses: rotation = 49758.7890625 \n",
      "Iteration = 740, Losses: rotation = 49758.48828125 \n",
      "Iteration = 750, Losses: rotation = 49758.1953125 \n",
      "Iteration = 760, Losses: rotation = 49757.9140625 \n",
      "Iteration = 770, Losses: rotation = 49757.65234375 \n",
      "Iteration = 780, Losses: rotation = 49757.390625 \n",
      "Iteration = 790, Losses: rotation = 49757.140625 \n",
      "Iteration = 800, Losses: rotation = 49756.91015625 \n",
      "Iteration = 810, Losses: rotation = 49756.6796875 \n",
      "Iteration = 820, Losses: rotation = 49756.46484375 \n",
      "Iteration = 830, Losses: rotation = 49756.2578125 \n",
      "Iteration = 840, Losses: rotation = 49756.0625 \n",
      "Iteration = 850, Losses: rotation = 49755.875 \n",
      "Iteration = 860, Losses: rotation = 49755.69140625 \n",
      "Iteration = 870, Losses: rotation = 49755.5234375 \n",
      "Iteration = 880, Losses: rotation = 49755.3671875 \n",
      "Iteration = 890, Losses: rotation = 49755.20703125 \n",
      "Iteration = 900, Losses: rotation = 49755.0625 \n",
      "Iteration = 910, Losses: rotation = 49754.92578125 \n",
      "Iteration = 920, Losses: rotation = 49754.79296875 \n"
     ]
    }
   ],
   "source": [
    "trained_model = train_models(data_loader=loader, train_itr=model_dict['train_itr'], metrics_dict=None, model_configs=model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b7a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c557468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
