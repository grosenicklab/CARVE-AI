{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca8353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f927c5d6",
   "metadata": {},
   "source": [
    "#### DEFINE DATA LOADER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f625ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE DATA LOADER FUNCTIONS ####\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "class DataGeneratorPPCA(Dataset):\n",
    "\n",
    "    def __init__(self, dims, hdims, min_sv=0.11, max_sv=5.0, sigma_sq=0.1, deterministic=True, total=10000):\n",
    "        self.dims = dims\n",
    "        self.hdims = hdims\n",
    "\n",
    "        self.eigs = min_sv + (max_sv - min_sv) * np.linspace(0, 1, hdims)\n",
    "        self.eigvectors = ortho_group.rvs(dims)[:, :hdims]\n",
    "        self.w = np.matmul(self.eigvectors, np.diag(np.sqrt(self.eigs - sigma_sq)))\n",
    "\n",
    "        self.sigma_sq = sigma_sq\n",
    "        self.sigma = np.sqrt(sigma_sq)\n",
    "\n",
    "        self.total = total\n",
    "        self.deterministic = deterministic\n",
    "        if self.deterministic:\n",
    "            self.z_sample = np.random.normal(size=(total, self.hdims))\n",
    "            self.x_sample = np.random.normal(np.matmul(self.z_sample, self.w.T), self.sigma).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.deterministic:\n",
    "            return self.x_sample[i]\n",
    "        else:\n",
    "            z_sample = np.random.normal(size=self.hdims)\n",
    "            return np.random.normal(self.w.dot(z_sample), self.sigma).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return a large number for an epoch\n",
    "        return self.total\n",
    "\n",
    "\n",
    "class DataGeneratorPCA(Dataset):\n",
    "    def __init__(self, dims, hdims, min_sv=0.11, max_sv=5.0, total=10000, sv_list=None,\n",
    "                 load_data=None):\n",
    "        self.dims = dims\n",
    "        self.hdims = hdims\n",
    "\n",
    "        if load_data is None:\n",
    "            if isinstance(sv_list, list):\n",
    "                assert len(sv_list) == dims\n",
    "                self.full_eigs = np.array(sorted(sv_list, reverse=True))\n",
    "            else:\n",
    "                self.full_eigs = min_sv + (max_sv - min_sv) * np.linspace(1, 0, dims)\n",
    "            self.eigs = self.full_eigs[:hdims]\n",
    "\n",
    "            self.full_svs = np.sqrt(self.full_eigs)\n",
    "\n",
    "            self.full_eigvectors = ortho_group.rvs(dims)\n",
    "            self.eigvectors = self.full_eigvectors[:, :hdims]\n",
    "\n",
    "            self.total = total\n",
    "\n",
    "            self.full_z_sample = np.random.normal(size=(total, self.dims))\n",
    "            self.x_sample = (self.full_eigvectors @ np.diag(self.full_svs) @ self.full_z_sample.T).T.astype(np.float32)\n",
    "\n",
    "        else:\n",
    "            self.x_sample = load_data\n",
    "            u, s, vh = np.linalg.svd(self.x_sample.T, full_matrices=False)\n",
    "            self.eigs = s[:self.hdims]\n",
    "            self.eigvectors = u[:, :self.hdims]\n",
    "            self.total = len(self.x_sample)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x_sample[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.x_sample.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef36b5",
   "metadata": {},
   "source": [
    "#### DEFINE MODEL CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe7f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE MODEL CLASSES ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self, model_name, model_type, model_class, input_dim, hidden_dim, init_scale, optim_class, lr,\n",
    "                 extra_model_args={}, extra_optim_args={}):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model_class = model_class\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_scale = init_scale\n",
    "        self.extra_model_args = extra_model_args\n",
    "\n",
    "        self.optim_class = optim_class\n",
    "        self.lr = lr\n",
    "        self.extra_optim_args = extra_optim_args\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model_class(input_dim=input_dim, hidden_dim=hidden_dim, init_scale=init_scale, **extra_model_args).to(device)\n",
    "\n",
    "        self.optimizer = optim_class(self.model.parameters(), lr=lr, **extra_optim_args)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self.model_type\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "class LinearAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, hidden_dim, init_scale=0.001,\n",
    "                 weight_reg_type=None, l2_reg_list=None):\n",
    "        super(LinearAE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "\n",
    "        self.weight_reg_type = weight_reg_type\n",
    "        self.l2_reg_scalar = None\n",
    "        self.l2_reg_list = l2_reg_list\n",
    "\n",
    "        self.encoder.weight.data.normal_(0.0, init_scale)\n",
    "        self.decoder.weight.data.normal_(0.0, init_scale)\n",
    "\n",
    "        # configure regularization parameters\n",
    "\n",
    "        assert self.weight_reg_type is None or isinstance(self.l2_reg_list, list), \\\n",
    "            \"l2_reg_list must be a list if weight_reg_type is not None\"\n",
    "\n",
    "        assert self.l2_reg_list is None or len(self.l2_reg_list) == hidden_dim, \\\n",
    "            \"Length of l2_reg_list must match latent dimension\"\n",
    "\n",
    "        if weight_reg_type in (\"uniform_product\", \"uniform_sum\"):\n",
    "            self.l2_reg_scalar = l2_reg_list[0] ** 2    # more efficient to use scalar than diag_weights\n",
    "\n",
    "        elif weight_reg_type == \"non_uniform_sum\":\n",
    "            self.reg_weights = torch.tensor(\n",
    "                np.array(self.l2_reg_list).astype(np.float32)\n",
    "            )\n",
    "            self.diag_weights = nn.Parameter(torch.diag(self.reg_weights), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.get_reconstruction_loss(x) + self._get_reg_loss()\n",
    "\n",
    "    def compute_trace_norm(self):\n",
    "        \"\"\"\n",
    "        Computes the trace norm of the autoencoder, as well as decoder and encoder individually\n",
    "        :return: trace_norm(W2W1), trace_norm(W1), trace_norm(W2)\n",
    "        \"\"\"\n",
    "        return torch.matmul(self.decoder.weight, self.encoder.weight).norm(p='nuc'), \\\n",
    "               self.encoder.weight.norm(p='nuc'), \\\n",
    "               self.decoder.weight.norm(p='nuc'),\n",
    "\n",
    "    def get_reconstruction_loss(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "\n",
    "        recon_loss = torch.sum((x - recon) ** 2) / len(x)\n",
    "        return recon_loss\n",
    "\n",
    "    def get_reg_weights_np(self):\n",
    "        if self.weight_reg_type is None:\n",
    "            return np.zeros(self.hidden_dim)\n",
    "        return np.array(self.l2_reg_list)\n",
    "\n",
    "    def _get_reg_loss(self):\n",
    "        # Standard L2 regularization, applied to W2W1 (product loss)\n",
    "        if self.weight_reg_type == 'uniform_product':\n",
    "            return self.l2_reg_scalar * (torch.norm(torch.matmul(self.decoder.weight, self.encoder.weight)) ** 2)\n",
    "\n",
    "        # Standard L2 regularization for encoder and decoder separately (sum loss)\n",
    "        elif self.weight_reg_type == 'uniform_sum':\n",
    "            # regularize both encoder and decoder\n",
    "            return self.l2_reg_scalar * (torch.norm(self.encoder.weight) ** 2 + torch.norm(self.decoder.weight) ** 2)\n",
    "\n",
    "        # non-uniform sum\n",
    "        elif self.weight_reg_type == 'non_uniform_sum':\n",
    "            return torch.norm(self.diag_weights @ self.encoder.weight) ** 2 \\\n",
    "                   + torch.norm(self.decoder.weight @ self.diag_weights) ** 2\n",
    "\n",
    "        # Do not apply regularization\n",
    "        elif self.weight_reg_type is None:\n",
    "            return 0.0\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"weight_reg_type should be one of (uniform_product, uniform_sum, non_uniform_sum, None)\")\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "class LinearAENestedDropout(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, hidden_dim, init_scale=0.001, prior_probs=None, use_expectation=False):\n",
    "        super(LinearAENestedDropout, self).__init__()\n",
    "\n",
    "        self.use_expectation = use_expectation\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "\n",
    "        self.encoder.weight.data.normal_(0.0, init_scale)\n",
    "        self.decoder.weight.data.normal_(0.0, init_scale)\n",
    "\n",
    "        if prior_probs is None:\n",
    "            # use geometric distribution\n",
    "            # p(b) = rho^b (1 - rho) (b = 0 ... k - 2)\n",
    "            # p(b = k-1) = 1 - sum(p(b), b < k-1)\n",
    "\n",
    "            self.geom_p = 0.9\n",
    "            prior_probs = [self.geom_p ** b * (1 - self.geom_p) for b in range(self.hidden_dim - 1)]\n",
    "            prior_probs.append(1.0 - sum(prior_probs))\n",
    "\n",
    "        self.prior_probs = torch.tensor(prior_probs)\n",
    "\n",
    "        cum_probs = [1. - sum(prior_probs[:i]) for i in range(self.hidden_dim)]\n",
    "        self.cum_probs = torch.tensor(cum_probs)\n",
    "        self.diag_expected_mask = nn.Parameter(torch.diag(self.cum_probs), requires_grad=False)\n",
    "        l_expected_mask = np.zeros((self.hidden_dim, self.hidden_dim))\n",
    "        for i in range(self.hidden_dim):\n",
    "            l_expected_mask[i, i] = cum_probs[i]\n",
    "            l_expected_mask[:i, i] = cum_probs[i]\n",
    "            l_expected_mask[i, :i] = cum_probs[i]\n",
    "        self.l_expected_mask = nn.Parameter(torch.from_numpy(l_expected_mask).float(), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_expectation:\n",
    "            tr_xtx = torch.norm(x) ** 2\n",
    "            w1_x = self.encoder(x).T        # (k, n)\n",
    "            tr_xt_w2_y = torch.trace(w1_x @ x @ self.decoder.weight @ self.diag_expected_mask)\n",
    "            w2t_w2_masked = (self.decoder.weight.T @ self.decoder.weight) * self.l_expected_mask\n",
    "            tr_yt_w2t_w2_y = torch.trace(w1_x @ w1_x.T @ w2t_w2_masked)\n",
    "\n",
    "            recon_loss = (tr_xtx - 2 * tr_xt_w2_y + tr_yt_w2t_w2_y) / len(x)\n",
    "        else:\n",
    "            hidden_units = self.encoder(x)\n",
    "            hidden_units = self._nested_dropout(hidden_units)\n",
    "            recon = self.decoder(hidden_units)\n",
    "\n",
    "            recon_loss = torch.sum((x - recon) ** 2) / len(x)\n",
    "        return recon_loss\n",
    "\n",
    "    def _nested_dropout(self, hidden_units):\n",
    "        prior_inds = torch.multinomial(self.prior_probs, len(hidden_units), replacement=True)\n",
    "        mask = torch.ones_like(hidden_units)\n",
    "        for hdim_i in range(1, self.hidden_dim):\n",
    "            drop_row_inds = (prior_inds < hdim_i).float()     # 1 if row is dropped, 0 if kept\n",
    "            mask[:, hdim_i] = 1 - drop_row_inds     # 1 if kept, 0 if dropped\n",
    "\n",
    "        masked_hidden_units = hidden_units * mask\n",
    "        return masked_hidden_units\n",
    "\n",
    "    def get_reconstruction_loss(self, x):\n",
    "        hidden_units = self.encoder(x)\n",
    "        recon = self.decoder(hidden_units)\n",
    "\n",
    "        recon_loss = torch.sum((x - recon) ** 2) / len(x)\n",
    "        return recon_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6de528",
   "metadata": {},
   "source": [
    "#### DEFINE MODEL TRAINING FUNCTION train_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a2d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE MODEL TRAINING FUNCTION train_models ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_models(data_loader, train_itr, metrics_dict, model_configs, eval_metrics_list=None):\n",
    "    # Initialize model\n",
    "#     print('new')\n",
    "#     model = model_config.get_model()\n",
    "#     optimizer = model_config.get_optimizer()\n",
    "    \n",
    "    for train_i in range(train_itr):\n",
    "        for x in data_loader:\n",
    "            x_cuda = x.to(device)\n",
    "\n",
    "            # ---- Optimize ----\n",
    "            losses = {}\n",
    "\n",
    "            model = model_config.get_model()\n",
    "            optimizer = model_config.get_optimizer()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(x_cuda)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            if model_config.type == 'rotation':\n",
    "#                 print('Before rotation: ', model.decoder.weight.grad)\n",
    "\n",
    "                # Rotation Augmented Gradient (RAG) \n",
    "                y = model.encoder.weight @ x_cuda.T\n",
    "                yy_t_norm = y @ y.T / float(len(x))\n",
    "                yy_t_upper = yy_t_norm - yy_t_norm.tril()\n",
    "                gamma = 0.5 * (yy_t_upper - yy_t_upper.T)\n",
    "#                 print('Gamma: ', gamma)\n",
    "                model.encoder.weight.grad -= gamma @ model.encoder.weight\n",
    "                model.decoder.weight.grad -= model.decoder.weight @ gamma.T\n",
    "\n",
    "#                 print('After rotation: ', model.decoder.weight.grad)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[model_config.name] = loss.item()\n",
    "\n",
    "        # ---- Log statistics ----\n",
    "        if train_i == 0 or (train_i + 1) % 10 == 0:\n",
    "            print(\"\".join([\"Iteration = {}, Losses: \".format(train_i + 1)]\n",
    "                          + [\"{} = {} \".format(key, val) for key, val in losses.items()]))\n",
    "            \n",
    "#     model_config.model = model\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df70f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=next(enumerate(loader))[1]\n",
    "# x_cuda = x.to(device)\n",
    "# print(model.encoder.weight.grad)\n",
    "# y = model.encoder.weight @ x_cuda.T\n",
    "# yy_t_norm = y @ y.T / float(len(x))\n",
    "# yy_t_upper = yy_t_norm - yy_t_norm.tril()\n",
    "# gamma = 0.5 * (yy_t_upper - yy_t_upper.T)\n",
    "# model.encoder.weight.grad -= gamma @ model.encoder.weight\n",
    "# print(model.encoder.weight.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b8d4a",
   "metadata": {},
   "source": [
    "#### DEFINE EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8791cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_weight_tensor_from_seq(weight_seq):\n",
    "    if isinstance(weight_seq, nn.Linear):\n",
    "        return weight_seq.weight.detach()\n",
    "    elif isinstance(weight_seq, nn.Sequential):\n",
    "        weight_tensor = None\n",
    "        for layer in weight_seq:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer_weight = layer.weight.detach()\n",
    "                if weight_tensor is None:\n",
    "                    weight_tensor = layer_weight\n",
    "                else:\n",
    "                    weight_tensor = layer_weight @ weight_tensor\n",
    "            elif isinstance(layer, nn.BatchNorm1d):\n",
    "                bn_weight = layer.weight.detach()\n",
    "\n",
    "                # ignore bias\n",
    "\n",
    "                if weight_tensor is None:\n",
    "                    weight_tensor = torch.diag(bn_weight)\n",
    "                else:\n",
    "                    weight_tensor = torch.diag(bn_weight) @ weight_tensor\n",
    "            else:\n",
    "                raise ValueError(\"Layer type {} not supported!\".format(type(layer)))\n",
    "        return weight_tensor\n",
    "\n",
    "\n",
    "def metric_transpose_theorem(model):\n",
    "    \"\"\"\n",
    "    Metric for how close encoder and decoder.T are\n",
    "    :param model: LinearAE model\n",
    "    :return: ||W1 - W2^T||_F^2 / hidden_dim\n",
    "    \"\"\"\n",
    "    encoder_weight = get_weight_tensor_from_seq(model.encoder)\n",
    "    decoder_weight = get_weight_tensor_from_seq(model.decoder)\n",
    "\n",
    "    transpose_metric = torch.norm(encoder_weight - decoder_weight.T) ** 2\n",
    "    return transpose_metric.item() / float(model.hidden_dim)\n",
    "\n",
    "\n",
    "def metric_alignment(model, gt_eigvectors):\n",
    "    \"\"\"\n",
    "    Metric for alignment of decoder columns to ground truth eigenvectors\n",
    "    :param model: Linear AE model\n",
    "    :param gt_eigvectors: ground truth eigenvectors (input_dims,hidden_dims)\n",
    "    :return: sum_i (1 - max_j (cos(eigvector_i, normalized_decoder column_j)))\n",
    "    \"\"\"\n",
    "    decoder_weight = get_weight_tensor_from_seq(model.decoder)\n",
    "    decoder_np = decoder_weight.detach().cpu().numpy()\n",
    "\n",
    "    # normalize columns of gt_eigvectors\n",
    "    norm_gt_eigvectors = gt_eigvectors / np.linalg.norm(gt_eigvectors, axis=0)\n",
    "    # normalize columns of decoder\n",
    "    norm_decoder = decoder_np / (np.linalg.norm(decoder_np, axis=0) + 1e-8)\n",
    "\n",
    "    total_angles = 0.0\n",
    "    for eig_i in range(gt_eigvectors.shape[1]):\n",
    "        eigvector = norm_gt_eigvectors[:, eig_i]\n",
    "        total_angles += 1. - np.max(np.abs(norm_decoder.T @ eigvector)) ** 2\n",
    "\n",
    "    return total_angles / float(model.hidden_dim)\n",
    "\n",
    "\n",
    "def metric_subspace(model, gt_eigvectors, gt_eigs):\n",
    "    decoder_weight = get_weight_tensor_from_seq(model.decoder)\n",
    "    decoder_np = decoder_weight.detach().cpu().numpy()\n",
    "\n",
    "    # k - tr(UU^T WW^T), where W is left singular vector matrix of decoder\n",
    "    u, s, vh = np.linalg.svd(decoder_np, full_matrices=False)\n",
    "    return 1 - np.trace(gt_eigvectors @ gt_eigvectors.T @ u @ u.T) / float(model.hidden_dim)\n",
    "\n",
    "\n",
    "def metric_loss(model, data_loader):\n",
    "    \"\"\"\n",
    "    Measures the full batch loss\n",
    "    :param model: a linear (variational) AE model\n",
    "    :param data_loader: full batch data loader. Should be different from the training data loader, if in minibatch mode\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    for x in data_loader:\n",
    "        loss = model(x.to(device)).item()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def metric_recon_loss(model, data_loader):\n",
    "    recon_loss = None\n",
    "    for x in data_loader:\n",
    "        recon_loss = model.get_reconstruction_loss(x.to(device)).item()\n",
    "    return recon_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb5912",
   "metadata": {},
   "source": [
    "### TRAIN A MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657aac",
   "metadata": {},
   "source": [
    "####  Model #1: Get the data and define the model - rotation with 400 hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430cf4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'rotation', 'model_type': 'rotation', 'model_class': <class '__main__.LinearAE'>, 'extra_model_args': {'weight_reg_type': None}, 'input_dim': 1000, 'hidden_dim': 400, 'init_scale': 0.0001, 'optim_class': <class 'torch.optim.sgd.SGD'>, 'extra_optim_args': {'momentum': 0.9, 'nesterov': True}, 'lr': 0.0001, 'train_itr': 1000, 'seed': 1234} \n",
      "\n",
      "LinearAE(\n",
      "  (encoder): Linear(in_features=1000, out_features=400, bias=False)\n",
      "  (decoder): Linear(in_features=400, out_features=1000, bias=False)\n",
      ") \n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0\n",
      ")\n",
      "Transpose: 1.9994410686194896e-05 \n",
      "\n",
      "Distance to axis-aligned solution: 0.9897644591560325 \n",
      "\n",
      "Distance to optimal subspace): 0.6001819992065429 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### GET DATA ####\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed=1234\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_dim = 1000\n",
    "hidden_dim = 400\n",
    "\n",
    "n_data = 5000\n",
    "batch_size = n_data\n",
    "\n",
    "max_sv = float(input_dim) * 0.1\n",
    "min_sv = 1.0\n",
    "sigma = 0.5\n",
    "\n",
    "gt_data = DataGeneratorPCA(input_dim, hidden_dim, min_sv=min_sv, max_sv=max_sv, total=n_data)\n",
    "data = DataGeneratorPCA(input_dim, hidden_dim, load_data=gt_data.x_sample)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#### Define the model ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### DEFINE MODEL #####\n",
    "model_dict = dict(\n",
    "    model_name='rotation',\n",
    "    model_type='rotation',\n",
    "    model_class=LinearAE,\n",
    "    extra_model_args = {\"weight_reg_type\": None},\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    init_scale=0.0001,\n",
    "    optim_class=torch.optim.SGD,\n",
    "    extra_optim_args={'momentum': 0.9, 'nesterov': True},\n",
    "    lr=0.0001,\n",
    "#     optim_class=torch.optim.Adam,\n",
    "#     extra_optim_args={},\n",
    "#     lr=0.0003,\n",
    "    train_itr=1000,#50000,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# model config contains the model \n",
    "model_config = ModelConfig(\n",
    "        model_name=model_dict['model_name'],\n",
    "        model_type=model_dict['model_type'],\n",
    "        model_class=model_dict['model_class'],\n",
    "        input_dim=model_dict['input_dim'], \n",
    "        hidden_dim=model_dict['hidden_dim'],\n",
    "        init_scale=model_dict['init_scale'],\n",
    "        extra_model_args=model_dict['extra_model_args'],\n",
    "        optim_class=model_dict['optim_class'],\n",
    "        lr=model_dict['lr'],\n",
    "        extra_optim_args=model_dict['extra_optim_args']\n",
    "    )\n",
    "\n",
    "print(model_dict,'\\n')\n",
    "print(model_config.get_model(),'\\n')\n",
    "print(model_config.get_optimizer())\n",
    "\n",
    "print('Transpose:', metric_transpose_theorem(model_config.get_model()),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(model_config.get_model(), data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(model_config.get_model(), data.eigvectors, data.eigs),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b744376",
   "metadata": {},
   "source": [
    "#### Model #1: Run the model - rotation with 400 hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d252037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstrution Loss: 47806.57421875 \n",
      "\n",
      "Loss: 49319.0234375 \n",
      "\n",
      "Transpose: 0.003536132350564003 \n",
      "\n",
      "Distance to axis-aligned solution: 0.09904575485510528 \n",
      "\n",
      "Distance to optimal subspace): 3.8146972658470446e-07 \n",
      "\n",
      "Iteration = 1, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 10, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 20, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 30, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 40, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 50, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 60, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 70, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 80, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 90, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 100, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 110, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 120, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 130, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 140, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 150, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 160, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 170, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 180, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 190, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 200, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 210, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 220, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 230, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 240, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 250, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 260, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 270, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 280, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 290, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 300, Losses: nd_expectation = 49319.015625 \n",
      "Iteration = 310, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 320, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 330, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 340, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 350, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 360, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 370, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 380, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 390, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 400, Losses: nd_expectation = 49319.015625 \n",
      "Iteration = 410, Losses: nd_expectation = 49319.015625 \n",
      "Iteration = 420, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 430, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 440, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 450, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 460, Losses: nd_expectation = 49319.015625 \n",
      "Iteration = 470, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 480, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 490, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 500, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 510, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 520, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 530, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 540, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 550, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 560, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 570, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 580, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 590, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 600, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 610, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 620, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 630, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 640, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 650, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 660, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 670, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 680, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 690, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 700, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 710, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 720, Losses: nd_expectation = 49319.015625 \n",
      "Iteration = 730, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 740, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 750, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 760, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 770, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 780, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 790, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 800, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 810, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 820, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 830, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 840, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 850, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 860, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 870, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 880, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 890, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 900, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 910, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 920, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 930, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 940, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 950, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 960, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 970, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 980, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 990, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1000, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1010, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 1020, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1030, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1040, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1050, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1060, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 1070, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1080, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1090, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1100, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1110, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1120, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1130, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1140, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1150, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1160, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1170, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1180, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1190, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1200, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1210, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1220, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1230, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1240, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1250, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1260, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1270, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1280, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1290, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1300, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1310, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1320, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1330, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1340, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1350, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1360, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1370, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1380, Losses: nd_expectation = 49319.03125 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1390, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1400, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1410, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1420, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 1430, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1440, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 1450, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1460, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1470, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1480, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1490, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1500, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1510, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1520, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1530, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1540, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1550, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1560, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1570, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1580, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1590, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1600, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1610, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1620, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1630, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1640, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1650, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1660, Losses: nd_expectation = 49319.05078125 \n",
      "Iteration = 1670, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1680, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1690, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1700, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1710, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1720, Losses: nd_expectation = 49319.05859375 \n",
      "Iteration = 1730, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 1740, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1750, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 1760, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1770, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 1780, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1790, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 1800, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1810, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1820, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1830, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1840, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1850, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 1860, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 1870, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1880, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1890, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 1900, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1910, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1920, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1930, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1940, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 1950, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 1960, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1970, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 1980, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 1990, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2000, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2010, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2020, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2030, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2040, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2050, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2060, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2070, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2080, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2090, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2100, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2110, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2120, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2130, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2140, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2150, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2160, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2170, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2180, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2190, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2200, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2210, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2220, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2230, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2240, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2250, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2260, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2270, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2280, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2290, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2300, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2310, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2320, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2330, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2340, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2350, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2360, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2370, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2380, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2390, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2400, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2410, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2420, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2430, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2440, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2450, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2460, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2470, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2480, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2490, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2500, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2510, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2520, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2530, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2540, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2550, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2560, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2570, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2580, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2590, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2600, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2610, Losses: nd_expectation = 49319.05078125 \n",
      "Iteration = 2620, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2630, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2640, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2650, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2660, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2670, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2680, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2690, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2700, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2710, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2720, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2730, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2740, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2750, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2760, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2770, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2780, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2790, Losses: nd_expectation = 49319.0390625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 2800, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2810, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2820, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2830, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2840, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2850, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2860, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2870, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2880, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2890, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2900, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2910, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2920, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2930, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2940, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2950, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2960, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2970, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2980, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2990, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 3000, Losses: nd_expectation = 49319.03515625 \n",
      "Reconstrution Loss: 47806.62890625 \n",
      "\n",
      "Loss: 49319.03515625 \n",
      "\n",
      "Transpose: 0.00029654598329216243 \n",
      "\n",
      "Distance to axis-aligned solution: 0.025499498070972848 \n",
      "\n",
      "Distance to optimal subspace): 8.583068847878295e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelIn = model_config.get_model()\n",
    "print('Reconstrution Loss:', metric_recon_loss(modelIn, loader),'\\n') # full batch loss\n",
    "print('Loss:', metric_loss(modelIn, loader),'\\n')\n",
    "print('Transpose:', metric_transpose_theorem(modelIn),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(modelIn, data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(modelIn, data.eigvectors, data.eigs),'\\n')\n",
    "\n",
    "trained_model = train_models(data_loader=loader, train_itr=model_dict['train_itr'], metrics_dict=None, model_configs=model_config)\n",
    "\n",
    "modelIn = model_config.get_model()\n",
    "print('Reconstrution Loss:', metric_recon_loss(modelIn, loader),'\\n') # full batch loss\n",
    "print('Loss:', metric_loss(modelIn, loader),'\\n')\n",
    "print('Transpose:', metric_transpose_theorem(modelIn),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(modelIn, data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(modelIn, data.eigvectors, data.eigs),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759a4f8",
   "metadata": {},
   "source": [
    "#### Model #2: Get the data and define the model - rotation with 50 hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facab280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'rotation', 'model_type': 'rotation', 'model_class': <class '__main__.LinearAE'>, 'extra_model_args': {'weight_reg_type': None}, 'input_dim': 1000, 'hidden_dim': 50, 'init_scale': 0.0001, 'optim_class': <class 'torch.optim.sgd.SGD'>, 'extra_optim_args': {'momentum': 0.9, 'nesterov': True}, 'lr': 0.0001, 'train_itr': 1000, 'seed': 1234} \n",
      "\n",
      "LinearAE(\n",
      "  (encoder): Linear(in_features=1000, out_features=50, bias=False)\n",
      "  (decoder): Linear(in_features=50, out_features=1000, bias=False)\n",
      ") \n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0\n",
      ")\n",
      "Transpose: 1.9993053283542393e-05 \n",
      "\n",
      "Distance to axis-aligned solution: 0.993878737828118 \n",
      "\n",
      "Distance to optimal subspace): 0.9511795520782471 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### GET DATA ####\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed=1234\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_dim = 1000\n",
    "hidden_dim = 50\n",
    "\n",
    "n_data = 5000\n",
    "batch_size = n_data\n",
    "\n",
    "max_sv = float(input_dim) * 0.1\n",
    "min_sv = 1.0\n",
    "sigma = 0.5\n",
    "\n",
    "gt_data = DataGeneratorPCA(input_dim, hidden_dim, min_sv=min_sv, max_sv=max_sv, total=n_data)\n",
    "data = DataGeneratorPCA(input_dim, hidden_dim, load_data=gt_data.x_sample)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#### Define the model ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### DEFINE MODEL #####\n",
    "model_dict = dict(\n",
    "    model_name='rotation',\n",
    "    model_type='rotation',\n",
    "    model_class=LinearAE,\n",
    "    extra_model_args = {\"weight_reg_type\": None},\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    init_scale=0.0001,\n",
    "    optim_class=torch.optim.SGD,\n",
    "    extra_optim_args={'momentum': 0.9, 'nesterov': True},\n",
    "    lr=0.0001,\n",
    "#     optim_class=torch.optim.Adam,\n",
    "#     extra_optim_args={},\n",
    "#     lr=0.0003,\n",
    "    train_itr=1000,#50000,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# model config contains the model \n",
    "model_config = ModelConfig(\n",
    "        model_name=model_dict['model_name'],\n",
    "        model_type=model_dict['model_type'],\n",
    "        model_class=model_dict['model_class'],\n",
    "        input_dim=model_dict['input_dim'], \n",
    "        hidden_dim=model_dict['hidden_dim'],\n",
    "        init_scale=model_dict['init_scale'],\n",
    "        extra_model_args=model_dict['extra_model_args'],\n",
    "        optim_class=model_dict['optim_class'],\n",
    "        lr=model_dict['lr'],\n",
    "        extra_optim_args=model_dict['extra_optim_args']\n",
    "    )\n",
    "\n",
    "print(model_dict,'\\n')\n",
    "print(model_config.get_model(),'\\n')\n",
    "print(model_config.get_optimizer())\n",
    "\n",
    "print('Transpose:', metric_transpose_theorem(model_config.get_model()),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(model_config.get_model(), data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(model_config.get_model(), data.eigvectors, data.eigs),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc0c9d",
   "metadata": {},
   "source": [
    "#### Model #2: Run the model - rotation with 50 hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d0b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstrution Loss: 50555.9375 \n",
      "\n",
      "Loss: 50555.9375 \n",
      "\n",
      "Transpose: 1.9993053283542393e-05 \n",
      "\n",
      "Distance to axis-aligned solution: 0.993878737828118 \n",
      "\n",
      "Distance to optimal subspace): 0.9511795520782471 \n",
      "\n",
      "Iteration = 1, Losses: rotation = 50555.9375 \n",
      "Iteration = 10, Losses: rotation = 50555.86328125 \n",
      "Iteration = 20, Losses: rotation = 50555.3125 \n",
      "Iteration = 30, Losses: rotation = 50549.6328125 \n",
      "Iteration = 40, Losses: rotation = 50485.734375 \n",
      "Iteration = 50, Losses: rotation = 49784.1875 \n",
      "Iteration = 60, Losses: rotation = 46384.1953125 \n",
      "Iteration = 70, Losses: rotation = 44725.10546875 \n",
      "Iteration = 80, Losses: rotation = 44449.55859375 \n",
      "Iteration = 90, Losses: rotation = 44315.09765625 \n",
      "Iteration = 100, Losses: rotation = 44242.43359375 \n",
      "Iteration = 110, Losses: rotation = 44198.40234375 \n",
      "Iteration = 120, Losses: rotation = 44166.01171875 \n",
      "Iteration = 130, Losses: rotation = 44141.96875 \n",
      "Iteration = 140, Losses: rotation = 44123.3984375 \n",
      "Iteration = 150, Losses: rotation = 44108.734375 \n",
      "Iteration = 160, Losses: rotation = 44097.0 \n",
      "Iteration = 170, Losses: rotation = 44087.4609375 \n",
      "Iteration = 180, Losses: rotation = 44079.62890625 \n",
      "Iteration = 190, Losses: rotation = 44073.125 \n",
      "Iteration = 200, Losses: rotation = 44067.6640625 \n",
      "Iteration = 210, Losses: rotation = 44063.0546875 \n",
      "Iteration = 220, Losses: rotation = 44059.11328125 \n",
      "Iteration = 230, Losses: rotation = 44055.7265625 \n",
      "Iteration = 240, Losses: rotation = 44052.796875 \n",
      "Iteration = 250, Losses: rotation = 44050.25390625 \n",
      "Iteration = 260, Losses: rotation = 44048.0234375 \n",
      "Iteration = 270, Losses: rotation = 44046.06640625 \n",
      "Iteration = 280, Losses: rotation = 44044.34375 \n",
      "Iteration = 290, Losses: rotation = 44042.8203125 \n",
      "Iteration = 300, Losses: rotation = 44041.4609375 \n",
      "Iteration = 310, Losses: rotation = 44040.26171875 \n",
      "Iteration = 320, Losses: rotation = 44039.18359375 \n",
      "Iteration = 330, Losses: rotation = 44038.22265625 \n",
      "Iteration = 340, Losses: rotation = 44037.35546875 \n",
      "Iteration = 350, Losses: rotation = 44036.578125 \n",
      "Iteration = 360, Losses: rotation = 44035.87890625 \n",
      "Iteration = 370, Losses: rotation = 44035.2421875 \n",
      "Iteration = 380, Losses: rotation = 44034.6640625 \n",
      "Iteration = 390, Losses: rotation = 44034.14453125 \n",
      "Iteration = 400, Losses: rotation = 44033.66796875 \n",
      "Iteration = 410, Losses: rotation = 44033.23046875 \n",
      "Iteration = 420, Losses: rotation = 44032.828125 \n",
      "Iteration = 430, Losses: rotation = 44032.4609375 \n",
      "Iteration = 440, Losses: rotation = 44032.1171875 \n",
      "Iteration = 450, Losses: rotation = 44031.8046875 \n",
      "Iteration = 460, Losses: rotation = 44031.51171875 \n",
      "Iteration = 470, Losses: rotation = 44031.2421875 \n",
      "Iteration = 480, Losses: rotation = 44030.984375 \n",
      "Iteration = 490, Losses: rotation = 44030.75390625 \n",
      "Iteration = 500, Losses: rotation = 44030.53515625 \n",
      "Iteration = 510, Losses: rotation = 44030.328125 \n",
      "Iteration = 520, Losses: rotation = 44030.1328125 \n",
      "Iteration = 530, Losses: rotation = 44029.953125 \n",
      "Iteration = 540, Losses: rotation = 44029.77734375 \n",
      "Iteration = 550, Losses: rotation = 44029.61328125 \n",
      "Iteration = 560, Losses: rotation = 44029.45703125 \n",
      "Iteration = 570, Losses: rotation = 44029.3125 \n",
      "Iteration = 580, Losses: rotation = 44029.17578125 \n",
      "Iteration = 590, Losses: rotation = 44029.0390625 \n",
      "Iteration = 600, Losses: rotation = 44028.9140625 \n",
      "Iteration = 610, Losses: rotation = 44028.7890625 \n",
      "Iteration = 620, Losses: rotation = 44028.6796875 \n",
      "Iteration = 630, Losses: rotation = 44028.57421875 \n",
      "Iteration = 640, Losses: rotation = 44028.46875 \n",
      "Iteration = 650, Losses: rotation = 44028.36328125 \n",
      "Iteration = 660, Losses: rotation = 44028.265625 \n",
      "Iteration = 670, Losses: rotation = 44028.171875 \n",
      "Iteration = 680, Losses: rotation = 44028.078125 \n",
      "Iteration = 690, Losses: rotation = 44027.99609375 \n",
      "Iteration = 700, Losses: rotation = 44027.91796875 \n",
      "Iteration = 710, Losses: rotation = 44027.83203125 \n",
      "Iteration = 720, Losses: rotation = 44027.75390625 \n",
      "Iteration = 730, Losses: rotation = 44027.6796875 \n",
      "Iteration = 740, Losses: rotation = 44027.609375 \n",
      "Iteration = 750, Losses: rotation = 44027.5390625 \n",
      "Iteration = 760, Losses: rotation = 44027.47265625 \n",
      "Iteration = 770, Losses: rotation = 44027.4140625 \n",
      "Iteration = 780, Losses: rotation = 44027.34765625 \n",
      "Iteration = 790, Losses: rotation = 44027.28125 \n",
      "Iteration = 800, Losses: rotation = 44027.2265625 \n",
      "Iteration = 810, Losses: rotation = 44027.16796875 \n",
      "Iteration = 820, Losses: rotation = 44027.11328125 \n",
      "Iteration = 830, Losses: rotation = 44027.0625 \n",
      "Iteration = 840, Losses: rotation = 44027.01171875 \n",
      "Iteration = 850, Losses: rotation = 44026.9609375 \n",
      "Iteration = 860, Losses: rotation = 44026.9140625 \n",
      "Iteration = 870, Losses: rotation = 44026.8671875 \n",
      "Iteration = 880, Losses: rotation = 44026.82421875 \n",
      "Iteration = 890, Losses: rotation = 44026.77734375 \n",
      "Iteration = 900, Losses: rotation = 44026.734375 \n",
      "Iteration = 910, Losses: rotation = 44026.69921875 \n",
      "Iteration = 920, Losses: rotation = 44026.66015625 \n",
      "Iteration = 930, Losses: rotation = 44026.6171875 \n",
      "Iteration = 940, Losses: rotation = 44026.57421875 \n",
      "Iteration = 950, Losses: rotation = 44026.54296875 \n",
      "Iteration = 960, Losses: rotation = 44026.515625 \n",
      "Iteration = 970, Losses: rotation = 44026.48046875 \n",
      "Iteration = 980, Losses: rotation = 44026.4453125 \n",
      "Iteration = 990, Losses: rotation = 44026.4140625 \n",
      "Iteration = 1000, Losses: rotation = 44026.37890625 \n",
      "Reconstrution Loss: 44026.37890625 \n",
      "\n",
      "Loss: 44026.37890625 \n",
      "\n",
      "Transpose: 6.9371220888569954e-06 \n",
      "\n",
      "Distance to axis-aligned solution: 0.6839896868942504 \n",
      "\n",
      "Distance to optimal subspace): 0.01873413085937503 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelIn = model_config.get_model()\n",
    "print('Reconstrution Loss:', metric_recon_loss(modelIn, loader),'\\n') # full batch loss\n",
    "print('Loss:', metric_loss(modelIn, loader),'\\n')\n",
    "print('Transpose:', metric_transpose_theorem(modelIn),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(modelIn, data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(modelIn, data.eigvectors, data.eigs),'\\n')\n",
    "\n",
    "trained_model = train_models(data_loader=loader, train_itr=1000, metrics_dict=None, model_configs=model_config)\n",
    "\n",
    "modelIn = model_config.get_model()\n",
    "print('Reconstrution Loss:', metric_recon_loss(modelIn, loader),'\\n') # full batch loss\n",
    "print('Loss:', metric_loss(modelIn, loader),'\\n')\n",
    "print('Transpose:', metric_transpose_theorem(modelIn),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(modelIn, data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(modelIn, data.eigvectors, data.eigs),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40a8dc",
   "metadata": {},
   "source": [
    "#### Model #3: Get the data and define the model - nested dropout with 40 hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d25560b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'nd_expectation', 'model_type': 'nested_dropout', 'model_class': <class '__main__.LinearAENestedDropout'>, 'extra_model_args': {'use_expectation': True}, 'input_dim': 1000, 'hidden_dim': 20, 'init_scale': 0.0001, 'optim_class': <class 'torch.optim.adam.Adam'>, 'extra_optim_args': {}, 'lr': 0.003, 'train_itr': 3000, 'seed': 1234} \n",
      "\n",
      "LinearAENestedDropout(\n",
      "  (encoder): Linear(in_features=1000, out_features=20, bias=False)\n",
      "  (decoder): Linear(in_features=20, out_features=1000, bias=False)\n",
      ") \n",
      "\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.003\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Transpose: 2.013526827795431e-05 \n",
      "\n",
      "Distance to axis-aligned solution: 0.9951107563416652 \n",
      "\n",
      "Distance to optimal subspace): 0.9793644726276398 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### GET DATA ####\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed=1234\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_dim = 1000\n",
    "hidden_dim = 20\n",
    "\n",
    "n_data = 5000\n",
    "batch_size = n_data\n",
    "\n",
    "max_sv = float(input_dim) * 0.1\n",
    "min_sv = 1.0\n",
    "sigma = 0.5\n",
    "\n",
    "gt_data = DataGeneratorPCA(input_dim, hidden_dim, min_sv=min_sv, max_sv=max_sv, total=n_data)\n",
    "data = DataGeneratorPCA(input_dim, hidden_dim, load_data=gt_data.x_sample)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#### Define the model ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#### DEFINE MODEL #####\n",
    "model_dict = dict(\n",
    "    model_name='nd_expectation',\n",
    "    model_type='nested_dropout',\n",
    "    model_class=LinearAENestedDropout,\n",
    "    extra_model_args = {'use_expectation': True},\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    init_scale=0.0001,\n",
    "    optim_class=torch.optim.Adam,\n",
    "    extra_optim_args={},\n",
    "    lr=0.003,\n",
    "    train_itr=3000,#50000,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# model config contains the model \n",
    "model_config = ModelConfig(\n",
    "        model_name=model_dict['model_name'],\n",
    "        model_type=model_dict['model_type'],\n",
    "        model_class=model_dict['model_class'],\n",
    "        input_dim=model_dict['input_dim'], \n",
    "        hidden_dim=model_dict['hidden_dim'],\n",
    "        init_scale=model_dict['init_scale'],\n",
    "        extra_model_args=model_dict['extra_model_args'],\n",
    "        optim_class=model_dict['optim_class'],\n",
    "        lr=model_dict['lr'],\n",
    "        extra_optim_args=model_dict['extra_optim_args']\n",
    "    )\n",
    "\n",
    "print(model_dict,'\\n')\n",
    "print(model_config.get_model(),'\\n')\n",
    "print(model_config.get_optimizer())\n",
    "\n",
    "print('Transpose:', metric_transpose_theorem(model_config.get_model()),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(model_config.get_model(), data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(model_config.get_model(), data.eigvectors, data.eigs),'\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac5645",
   "metadata": {},
   "source": [
    "#### Model #3: Run the model - nested dropout with 40 hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab8b8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstrution Loss: 50555.9375 \n",
      "\n",
      "Loss: 50544.8828125 \n",
      "\n",
      "Transpose: 2.013526827795431e-05 \n",
      "\n",
      "Distance to axis-aligned solution: 0.9951107563416652 \n",
      "\n",
      "Distance to optimal subspace): 0.9793644726276398 \n",
      "\n",
      "Iteration = 1, Losses: nd_expectation = 50544.8828125 \n",
      "Iteration = 10, Losses: nd_expectation = 50005.375 \n",
      "Iteration = 20, Losses: nd_expectation = 49659.06640625 \n",
      "Iteration = 30, Losses: nd_expectation = 49504.6328125 \n",
      "Iteration = 40, Losses: nd_expectation = 49428.1875 \n",
      "Iteration = 50, Losses: nd_expectation = 49391.37109375 \n",
      "Iteration = 60, Losses: nd_expectation = 49371.125 \n",
      "Iteration = 70, Losses: nd_expectation = 49358.13671875 \n",
      "Iteration = 80, Losses: nd_expectation = 49350.11328125 \n",
      "Iteration = 90, Losses: nd_expectation = 49344.70703125 \n",
      "Iteration = 100, Losses: nd_expectation = 49340.89453125 \n",
      "Iteration = 110, Losses: nd_expectation = 49338.078125 \n",
      "Iteration = 120, Losses: nd_expectation = 49335.91796875 \n",
      "Iteration = 130, Losses: nd_expectation = 49334.1875 \n",
      "Iteration = 140, Losses: nd_expectation = 49332.765625 \n",
      "Iteration = 150, Losses: nd_expectation = 49331.5546875 \n",
      "Iteration = 160, Losses: nd_expectation = 49330.5234375 \n",
      "Iteration = 170, Losses: nd_expectation = 49329.625 \n",
      "Iteration = 180, Losses: nd_expectation = 49328.84375 \n",
      "Iteration = 190, Losses: nd_expectation = 49328.16015625 \n",
      "Iteration = 200, Losses: nd_expectation = 49327.55078125 \n",
      "Iteration = 210, Losses: nd_expectation = 49327.015625 \n",
      "Iteration = 220, Losses: nd_expectation = 49326.53515625 \n",
      "Iteration = 230, Losses: nd_expectation = 49326.09765625 \n",
      "Iteration = 240, Losses: nd_expectation = 49325.703125 \n",
      "Iteration = 250, Losses: nd_expectation = 49325.3359375 \n",
      "Iteration = 260, Losses: nd_expectation = 49325.0 \n",
      "Iteration = 270, Losses: nd_expectation = 49324.68359375 \n",
      "Iteration = 280, Losses: nd_expectation = 49324.390625 \n",
      "Iteration = 290, Losses: nd_expectation = 49324.11328125 \n",
      "Iteration = 300, Losses: nd_expectation = 49323.8515625 \n",
      "Iteration = 310, Losses: nd_expectation = 49323.60546875 \n",
      "Iteration = 320, Losses: nd_expectation = 49323.37109375 \n",
      "Iteration = 330, Losses: nd_expectation = 49323.15234375 \n",
      "Iteration = 340, Losses: nd_expectation = 49322.9453125 \n",
      "Iteration = 350, Losses: nd_expectation = 49322.74609375 \n",
      "Iteration = 360, Losses: nd_expectation = 49322.55859375 \n",
      "Iteration = 370, Losses: nd_expectation = 49322.37890625 \n",
      "Iteration = 380, Losses: nd_expectation = 49322.21484375 \n",
      "Iteration = 390, Losses: nd_expectation = 49322.0625 \n",
      "Iteration = 400, Losses: nd_expectation = 49321.91015625 \n",
      "Iteration = 410, Losses: nd_expectation = 49321.77734375 \n",
      "Iteration = 420, Losses: nd_expectation = 49321.6484375 \n",
      "Iteration = 430, Losses: nd_expectation = 49321.52734375 \n",
      "Iteration = 440, Losses: nd_expectation = 49321.4140625 \n",
      "Iteration = 450, Losses: nd_expectation = 49321.30859375 \n",
      "Iteration = 460, Losses: nd_expectation = 49321.21484375 \n",
      "Iteration = 470, Losses: nd_expectation = 49321.125 \n",
      "Iteration = 480, Losses: nd_expectation = 49321.04296875 \n",
      "Iteration = 490, Losses: nd_expectation = 49320.96484375 \n",
      "Iteration = 500, Losses: nd_expectation = 49320.89453125 \n",
      "Iteration = 510, Losses: nd_expectation = 49320.82421875 \n",
      "Iteration = 520, Losses: nd_expectation = 49320.76171875 \n",
      "Iteration = 530, Losses: nd_expectation = 49320.703125 \n",
      "Iteration = 540, Losses: nd_expectation = 49320.6484375 \n",
      "Iteration = 550, Losses: nd_expectation = 49320.59765625 \n",
      "Iteration = 560, Losses: nd_expectation = 49320.55078125 \n",
      "Iteration = 570, Losses: nd_expectation = 49320.50390625 \n",
      "Iteration = 580, Losses: nd_expectation = 49320.46484375 \n",
      "Iteration = 590, Losses: nd_expectation = 49320.421875 \n",
      "Iteration = 600, Losses: nd_expectation = 49320.3828125 \n",
      "Iteration = 610, Losses: nd_expectation = 49320.34375 \n",
      "Iteration = 620, Losses: nd_expectation = 49320.30859375 \n",
      "Iteration = 630, Losses: nd_expectation = 49320.27734375 \n",
      "Iteration = 640, Losses: nd_expectation = 49320.24609375 \n",
      "Iteration = 650, Losses: nd_expectation = 49320.21875 \n",
      "Iteration = 660, Losses: nd_expectation = 49320.1875 \n",
      "Iteration = 670, Losses: nd_expectation = 49320.16015625 \n",
      "Iteration = 680, Losses: nd_expectation = 49320.1328125 \n",
      "Iteration = 690, Losses: nd_expectation = 49320.10546875 \n",
      "Iteration = 700, Losses: nd_expectation = 49320.078125 \n",
      "Iteration = 710, Losses: nd_expectation = 49320.0546875 \n",
      "Iteration = 720, Losses: nd_expectation = 49320.03125 \n",
      "Iteration = 730, Losses: nd_expectation = 49320.0078125 \n",
      "Iteration = 740, Losses: nd_expectation = 49319.98828125 \n",
      "Iteration = 750, Losses: nd_expectation = 49319.96875 \n",
      "Iteration = 760, Losses: nd_expectation = 49319.9453125 \n",
      "Iteration = 770, Losses: nd_expectation = 49319.92578125 \n",
      "Iteration = 780, Losses: nd_expectation = 49319.90625 \n",
      "Iteration = 790, Losses: nd_expectation = 49319.88671875 \n",
      "Iteration = 800, Losses: nd_expectation = 49319.8671875 \n",
      "Iteration = 810, Losses: nd_expectation = 49319.84765625 \n",
      "Iteration = 820, Losses: nd_expectation = 49319.83203125 \n",
      "Iteration = 830, Losses: nd_expectation = 49319.81640625 \n",
      "Iteration = 840, Losses: nd_expectation = 49319.80078125 \n",
      "Iteration = 850, Losses: nd_expectation = 49319.78125 \n",
      "Iteration = 860, Losses: nd_expectation = 49319.76953125 \n",
      "Iteration = 870, Losses: nd_expectation = 49319.75 \n",
      "Iteration = 880, Losses: nd_expectation = 49319.73828125 \n",
      "Iteration = 890, Losses: nd_expectation = 49319.72265625 \n",
      "Iteration = 900, Losses: nd_expectation = 49319.70703125 \n",
      "Iteration = 910, Losses: nd_expectation = 49319.6953125 \n",
      "Iteration = 920, Losses: nd_expectation = 49319.6796875 \n",
      "Iteration = 930, Losses: nd_expectation = 49319.66796875 \n",
      "Iteration = 940, Losses: nd_expectation = 49319.65625 \n",
      "Iteration = 950, Losses: nd_expectation = 49319.640625 \n",
      "Iteration = 960, Losses: nd_expectation = 49319.62890625 \n",
      "Iteration = 970, Losses: nd_expectation = 49319.6171875 \n",
      "Iteration = 980, Losses: nd_expectation = 49319.6015625 \n",
      "Iteration = 990, Losses: nd_expectation = 49319.59375 \n",
      "Iteration = 1000, Losses: nd_expectation = 49319.58203125 \n",
      "Iteration = 1010, Losses: nd_expectation = 49319.56640625 \n",
      "Iteration = 1020, Losses: nd_expectation = 49319.55859375 \n",
      "Iteration = 1030, Losses: nd_expectation = 49319.546875 \n",
      "Iteration = 1040, Losses: nd_expectation = 49319.53515625 \n",
      "Iteration = 1050, Losses: nd_expectation = 49319.52734375 \n",
      "Iteration = 1060, Losses: nd_expectation = 49319.515625 \n",
      "Iteration = 1070, Losses: nd_expectation = 49319.50390625 \n",
      "Iteration = 1080, Losses: nd_expectation = 49319.49609375 \n",
      "Iteration = 1090, Losses: nd_expectation = 49319.484375 \n",
      "Iteration = 1100, Losses: nd_expectation = 49319.4765625 \n",
      "Iteration = 1110, Losses: nd_expectation = 49319.46484375 \n",
      "Iteration = 1120, Losses: nd_expectation = 49319.45703125 \n",
      "Iteration = 1130, Losses: nd_expectation = 49319.4453125 \n",
      "Iteration = 1140, Losses: nd_expectation = 49319.44140625 \n",
      "Iteration = 1150, Losses: nd_expectation = 49319.4296875 \n",
      "Iteration = 1160, Losses: nd_expectation = 49319.421875 \n",
      "Iteration = 1170, Losses: nd_expectation = 49319.4140625 \n",
      "Iteration = 1180, Losses: nd_expectation = 49319.40625 \n",
      "Iteration = 1190, Losses: nd_expectation = 49319.3984375 \n",
      "Iteration = 1200, Losses: nd_expectation = 49319.38671875 \n",
      "Iteration = 1210, Losses: nd_expectation = 49319.3828125 \n",
      "Iteration = 1220, Losses: nd_expectation = 49319.37109375 \n",
      "Iteration = 1230, Losses: nd_expectation = 49319.3671875 \n",
      "Iteration = 1240, Losses: nd_expectation = 49319.35546875 \n",
      "Iteration = 1250, Losses: nd_expectation = 49319.3515625 \n",
      "Iteration = 1260, Losses: nd_expectation = 49319.34375 \n",
      "Iteration = 1270, Losses: nd_expectation = 49319.3359375 \n",
      "Iteration = 1280, Losses: nd_expectation = 49319.328125 \n",
      "Iteration = 1290, Losses: nd_expectation = 49319.3203125 \n",
      "Iteration = 1300, Losses: nd_expectation = 49319.31640625 \n",
      "Iteration = 1310, Losses: nd_expectation = 49319.30859375 \n",
      "Iteration = 1320, Losses: nd_expectation = 49319.30078125 \n",
      "Iteration = 1330, Losses: nd_expectation = 49319.296875 \n",
      "Iteration = 1340, Losses: nd_expectation = 49319.2890625 \n",
      "Iteration = 1350, Losses: nd_expectation = 49319.28125 \n",
      "Iteration = 1360, Losses: nd_expectation = 49319.27734375 \n",
      "Iteration = 1370, Losses: nd_expectation = 49319.26953125 \n",
      "Iteration = 1380, Losses: nd_expectation = 49319.265625 \n",
      "Iteration = 1390, Losses: nd_expectation = 49319.26171875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1400, Losses: nd_expectation = 49319.25390625 \n",
      "Iteration = 1410, Losses: nd_expectation = 49319.24609375 \n",
      "Iteration = 1420, Losses: nd_expectation = 49319.24609375 \n",
      "Iteration = 1430, Losses: nd_expectation = 49319.23828125 \n",
      "Iteration = 1440, Losses: nd_expectation = 49319.234375 \n",
      "Iteration = 1450, Losses: nd_expectation = 49319.23046875 \n",
      "Iteration = 1460, Losses: nd_expectation = 49319.22265625 \n",
      "Iteration = 1470, Losses: nd_expectation = 49319.21875 \n",
      "Iteration = 1480, Losses: nd_expectation = 49319.21484375 \n",
      "Iteration = 1490, Losses: nd_expectation = 49319.2109375 \n",
      "Iteration = 1500, Losses: nd_expectation = 49319.2109375 \n",
      "Iteration = 1510, Losses: nd_expectation = 49319.203125 \n",
      "Iteration = 1520, Losses: nd_expectation = 49319.19921875 \n",
      "Iteration = 1530, Losses: nd_expectation = 49319.1953125 \n",
      "Iteration = 1540, Losses: nd_expectation = 49319.19140625 \n",
      "Iteration = 1550, Losses: nd_expectation = 49319.1875 \n",
      "Iteration = 1560, Losses: nd_expectation = 49319.1875 \n",
      "Iteration = 1570, Losses: nd_expectation = 49319.18359375 \n",
      "Iteration = 1580, Losses: nd_expectation = 49319.17578125 \n",
      "Iteration = 1590, Losses: nd_expectation = 49319.17578125 \n",
      "Iteration = 1600, Losses: nd_expectation = 49319.17578125 \n",
      "Iteration = 1610, Losses: nd_expectation = 49319.16796875 \n",
      "Iteration = 1620, Losses: nd_expectation = 49319.1640625 \n",
      "Iteration = 1630, Losses: nd_expectation = 49319.1640625 \n",
      "Iteration = 1640, Losses: nd_expectation = 49319.16015625 \n",
      "Iteration = 1650, Losses: nd_expectation = 49319.16015625 \n",
      "Iteration = 1660, Losses: nd_expectation = 49319.15625 \n",
      "Iteration = 1670, Losses: nd_expectation = 49319.15234375 \n",
      "Iteration = 1680, Losses: nd_expectation = 49319.1484375 \n",
      "Iteration = 1690, Losses: nd_expectation = 49319.1484375 \n",
      "Iteration = 1700, Losses: nd_expectation = 49319.14453125 \n",
      "Iteration = 1710, Losses: nd_expectation = 49319.140625 \n",
      "Iteration = 1720, Losses: nd_expectation = 49319.140625 \n",
      "Iteration = 1730, Losses: nd_expectation = 49319.140625 \n",
      "Iteration = 1740, Losses: nd_expectation = 49319.13671875 \n",
      "Iteration = 1750, Losses: nd_expectation = 49319.1328125 \n",
      "Iteration = 1760, Losses: nd_expectation = 49319.1328125 \n",
      "Iteration = 1770, Losses: nd_expectation = 49319.12890625 \n",
      "Iteration = 1780, Losses: nd_expectation = 49319.125 \n",
      "Iteration = 1790, Losses: nd_expectation = 49319.125 \n",
      "Iteration = 1800, Losses: nd_expectation = 49319.125 \n",
      "Iteration = 1810, Losses: nd_expectation = 49319.125 \n",
      "Iteration = 1820, Losses: nd_expectation = 49319.125 \n",
      "Iteration = 1830, Losses: nd_expectation = 49319.12109375 \n",
      "Iteration = 1840, Losses: nd_expectation = 49319.1171875 \n",
      "Iteration = 1850, Losses: nd_expectation = 49319.11328125 \n",
      "Iteration = 1860, Losses: nd_expectation = 49319.11328125 \n",
      "Iteration = 1870, Losses: nd_expectation = 49319.11328125 \n",
      "Iteration = 1880, Losses: nd_expectation = 49319.109375 \n",
      "Iteration = 1890, Losses: nd_expectation = 49319.109375 \n",
      "Iteration = 1900, Losses: nd_expectation = 49319.109375 \n",
      "Iteration = 1910, Losses: nd_expectation = 49319.10546875 \n",
      "Iteration = 1920, Losses: nd_expectation = 49319.10546875 \n",
      "Iteration = 1930, Losses: nd_expectation = 49319.10546875 \n",
      "Iteration = 1940, Losses: nd_expectation = 49319.10546875 \n",
      "Iteration = 1950, Losses: nd_expectation = 49319.1015625 \n",
      "Iteration = 1960, Losses: nd_expectation = 49319.1015625 \n",
      "Iteration = 1970, Losses: nd_expectation = 49319.09765625 \n",
      "Iteration = 1980, Losses: nd_expectation = 49319.09375 \n",
      "Iteration = 1990, Losses: nd_expectation = 49319.09375 \n",
      "Iteration = 2000, Losses: nd_expectation = 49319.09375 \n",
      "Iteration = 2010, Losses: nd_expectation = 49319.09375 \n",
      "Iteration = 2020, Losses: nd_expectation = 49319.08984375 \n",
      "Iteration = 2030, Losses: nd_expectation = 49319.08984375 \n",
      "Iteration = 2040, Losses: nd_expectation = 49319.08984375 \n",
      "Iteration = 2050, Losses: nd_expectation = 49319.08984375 \n",
      "Iteration = 2060, Losses: nd_expectation = 49319.08984375 \n",
      "Iteration = 2070, Losses: nd_expectation = 49319.0859375 \n",
      "Iteration = 2080, Losses: nd_expectation = 49319.0859375 \n",
      "Iteration = 2090, Losses: nd_expectation = 49319.08203125 \n",
      "Iteration = 2100, Losses: nd_expectation = 49319.0859375 \n",
      "Iteration = 2110, Losses: nd_expectation = 49319.08203125 \n",
      "Iteration = 2120, Losses: nd_expectation = 49319.08203125 \n",
      "Iteration = 2130, Losses: nd_expectation = 49319.08203125 \n",
      "Iteration = 2140, Losses: nd_expectation = 49319.078125 \n",
      "Iteration = 2150, Losses: nd_expectation = 49319.07421875 \n",
      "Iteration = 2160, Losses: nd_expectation = 49319.07421875 \n",
      "Iteration = 2170, Losses: nd_expectation = 49319.07421875 \n",
      "Iteration = 2180, Losses: nd_expectation = 49319.07421875 \n",
      "Iteration = 2190, Losses: nd_expectation = 49319.0703125 \n",
      "Iteration = 2200, Losses: nd_expectation = 49319.0703125 \n",
      "Iteration = 2210, Losses: nd_expectation = 49319.0703125 \n",
      "Iteration = 2220, Losses: nd_expectation = 49319.0703125 \n",
      "Iteration = 2230, Losses: nd_expectation = 49319.0703125 \n",
      "Iteration = 2240, Losses: nd_expectation = 49319.06640625 \n",
      "Iteration = 2250, Losses: nd_expectation = 49319.06640625 \n",
      "Iteration = 2260, Losses: nd_expectation = 49319.06640625 \n",
      "Iteration = 2270, Losses: nd_expectation = 49319.0625 \n",
      "Iteration = 2280, Losses: nd_expectation = 49319.06640625 \n",
      "Iteration = 2290, Losses: nd_expectation = 49319.0625 \n",
      "Iteration = 2300, Losses: nd_expectation = 49319.05859375 \n",
      "Iteration = 2310, Losses: nd_expectation = 49319.0625 \n",
      "Iteration = 2320, Losses: nd_expectation = 49319.05859375 \n",
      "Iteration = 2330, Losses: nd_expectation = 49319.05859375 \n",
      "Iteration = 2340, Losses: nd_expectation = 49319.05859375 \n",
      "Iteration = 2350, Losses: nd_expectation = 49319.05859375 \n",
      "Iteration = 2360, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2370, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2380, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2390, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2400, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2410, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2420, Losses: nd_expectation = 49319.0546875 \n",
      "Iteration = 2430, Losses: nd_expectation = 49319.05078125 \n",
      "Iteration = 2440, Losses: nd_expectation = 49319.05078125 \n",
      "Iteration = 2450, Losses: nd_expectation = 49319.05078125 \n",
      "Iteration = 2460, Losses: nd_expectation = 49319.05078125 \n",
      "Iteration = 2470, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2480, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2490, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2500, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2510, Losses: nd_expectation = 49319.046875 \n",
      "Iteration = 2520, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2530, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2540, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2550, Losses: nd_expectation = 49319.04296875 \n",
      "Iteration = 2560, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2570, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2580, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2590, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2600, Losses: nd_expectation = 49319.0390625 \n",
      "Iteration = 2610, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2620, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2630, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2640, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2650, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2660, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2670, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2680, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2690, Losses: nd_expectation = 49319.03515625 \n",
      "Iteration = 2700, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2710, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2720, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2730, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2740, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2750, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2760, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2770, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2780, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2790, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2800, Losses: nd_expectation = 49319.03125 \n",
      "Iteration = 2810, Losses: nd_expectation = 49319.02734375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 2820, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2830, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2840, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2850, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2860, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2870, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2880, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2890, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2900, Losses: nd_expectation = 49319.02734375 \n",
      "Iteration = 2910, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2920, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2930, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 2940, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2950, Losses: nd_expectation = 49319.01953125 \n",
      "Iteration = 2960, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2970, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2980, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 2990, Losses: nd_expectation = 49319.0234375 \n",
      "Iteration = 3000, Losses: nd_expectation = 49319.0234375 \n",
      "Reconstrution Loss: 47806.57421875 \n",
      "\n",
      "Loss: 49319.0234375 \n",
      "\n",
      "Transpose: 0.003536132350564003 \n",
      "\n",
      "Distance to axis-aligned solution: 0.09904575485510528 \n",
      "\n",
      "Distance to optimal subspace): 3.8146972658470446e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelIn = model_config.get_model()\n",
    "print('Reconstrution Loss:', metric_recon_loss(modelIn, loader),'\\n') # full batch loss\n",
    "print('Loss:', metric_loss(modelIn, loader),'\\n')\n",
    "print('Transpose:', metric_transpose_theorem(modelIn),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(modelIn, data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(modelIn, data.eigvectors, data.eigs),'\\n')\n",
    "\n",
    "trained_model = train_models(data_loader=loader, train_itr=model_dict['train_itr'], metrics_dict=None, model_configs=model_config)\n",
    "\n",
    "modelIn = model_config.get_model()\n",
    "print('Reconstrution Loss:', metric_recon_loss(modelIn, loader),'\\n') # full batch loss\n",
    "print('Loss:', metric_loss(modelIn, loader),'\\n')\n",
    "print('Transpose:', metric_transpose_theorem(modelIn),'\\n') # how close encoder and decoder.T are \n",
    "print('Distance to axis-aligned solution:', metric_alignment(modelIn, data.eigvectors),'\\n') # alignment of decoder columns to ground truth eigenvectors\n",
    "print('Distance to optimal subspace):', metric_subspace(modelIn, data.eigvectors, data.eigs),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae694dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386299b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682111a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
