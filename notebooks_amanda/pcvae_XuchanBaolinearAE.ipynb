{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f910cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d16679",
   "metadata": {},
   "source": [
    "#### DEFINE DATA LOADER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93d41d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE DATA LOADER FUNCTIONS ####\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "class DataGeneratorPPCA(Dataset):\n",
    "\n",
    "    def __init__(self, dims, hdims, min_sv=0.11, max_sv=5.0, sigma_sq=0.1, deterministic=True, total=10000):\n",
    "        self.dims = dims\n",
    "        self.hdims = hdims\n",
    "\n",
    "        self.eigs = min_sv + (max_sv - min_sv) * np.linspace(0, 1, hdims)\n",
    "        self.eigvectors = ortho_group.rvs(dims)[:, :hdims]\n",
    "        self.w = np.matmul(self.eigvectors, np.diag(np.sqrt(self.eigs - sigma_sq)))\n",
    "\n",
    "        self.sigma_sq = sigma_sq\n",
    "        self.sigma = np.sqrt(sigma_sq)\n",
    "\n",
    "        self.total = total\n",
    "        self.deterministic = deterministic\n",
    "        if self.deterministic:\n",
    "            self.z_sample = np.random.normal(size=(total, self.hdims))\n",
    "            self.x_sample = np.random.normal(np.matmul(self.z_sample, self.w.T), self.sigma).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.deterministic:\n",
    "            return self.x_sample[i]\n",
    "        else:\n",
    "            z_sample = np.random.normal(size=self.hdims)\n",
    "            return np.random.normal(self.w.dot(z_sample), self.sigma).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return a large number for an epoch\n",
    "        return self.total\n",
    "\n",
    "\n",
    "class DataGeneratorPCA(Dataset):\n",
    "    def __init__(self, dims, hdims, min_sv=0.11, max_sv=5.0, total=10000, sv_list=None,\n",
    "                 load_data=None):\n",
    "        self.dims = dims\n",
    "        self.hdims = hdims\n",
    "\n",
    "        if load_data is None:\n",
    "            if isinstance(sv_list, list):\n",
    "                assert len(sv_list) == dims\n",
    "                self.full_eigs = np.array(sorted(sv_list, reverse=True))\n",
    "            else:\n",
    "                self.full_eigs = min_sv + (max_sv - min_sv) * np.linspace(1, 0, dims)\n",
    "            self.eigs = self.full_eigs[:hdims]\n",
    "\n",
    "            self.full_svs = np.sqrt(self.full_eigs)\n",
    "\n",
    "            self.full_eigvectors = ortho_group.rvs(dims)\n",
    "            self.eigvectors = self.full_eigvectors[:, :hdims]\n",
    "\n",
    "            self.total = total\n",
    "\n",
    "            self.full_z_sample = np.random.normal(size=(total, self.dims))\n",
    "            self.x_sample = (self.full_eigvectors @ np.diag(self.full_svs) @ self.full_z_sample.T).T.astype(np.float32)\n",
    "\n",
    "        else:\n",
    "            self.x_sample = load_data\n",
    "            u, s, vh = np.linalg.svd(self.x_sample.T, full_matrices=False)\n",
    "            self.eigs = s[:self.hdims]\n",
    "            self.eigvectors = u[:, :self.hdims]\n",
    "            self.total = len(self.x_sample)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x_sample[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.x_sample.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf7535",
   "metadata": {},
   "source": [
    "#### DEFINE MODEL CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "921e06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE MODEL CLASSES ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self, model_name, model_type, model_class, input_dim, hidden_dim, init_scale, optim_class, lr,\n",
    "                 extra_model_args={}, extra_optim_args={}):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model_class = model_class\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_scale = init_scale\n",
    "        self.extra_model_args = extra_model_args\n",
    "\n",
    "        self.optim_class = optim_class\n",
    "        self.lr = lr\n",
    "        self.extra_optim_args = extra_optim_args\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model_class(input_dim=input_dim, hidden_dim=hidden_dim, init_scale=init_scale, **extra_model_args).to(device)\n",
    "\n",
    "        self.optimizer = optim_class(self.model.parameters(), lr=lr, **extra_optim_args)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self.model_type\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "class LinearAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, hidden_dim, init_scale=0.001,\n",
    "                 weight_reg_type=None, l2_reg_list=None):\n",
    "        super(LinearAE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "\n",
    "        self.weight_reg_type = weight_reg_type\n",
    "        self.l2_reg_scalar = None\n",
    "        self.l2_reg_list = l2_reg_list\n",
    "\n",
    "        self.encoder.weight.data.normal_(0.0, init_scale)\n",
    "        self.decoder.weight.data.normal_(0.0, init_scale)\n",
    "\n",
    "        # configure regularization parameters\n",
    "\n",
    "        assert self.weight_reg_type is None or isinstance(self.l2_reg_list, list), \\\n",
    "            \"l2_reg_list must be a list if weight_reg_type is not None\"\n",
    "\n",
    "        assert self.l2_reg_list is None or len(self.l2_reg_list) == hidden_dim, \\\n",
    "            \"Length of l2_reg_list must match latent dimension\"\n",
    "\n",
    "        if weight_reg_type in (\"uniform_product\", \"uniform_sum\"):\n",
    "            self.l2_reg_scalar = l2_reg_list[0] ** 2    # more efficient to use scalar than diag_weights\n",
    "\n",
    "        elif weight_reg_type == \"non_uniform_sum\":\n",
    "            self.reg_weights = torch.tensor(\n",
    "                np.array(self.l2_reg_list).astype(np.float32)\n",
    "            )\n",
    "            self.diag_weights = nn.Parameter(torch.diag(self.reg_weights), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.get_reconstruction_loss(x) + self._get_reg_loss()\n",
    "\n",
    "    def compute_trace_norm(self):\n",
    "        \"\"\"\n",
    "        Computes the trace norm of the autoencoder, as well as decoder and encoder individually\n",
    "        :return: trace_norm(W2W1), trace_norm(W1), trace_norm(W2)\n",
    "        \"\"\"\n",
    "        return torch.matmul(self.decoder.weight, self.encoder.weight).norm(p='nuc'), \\\n",
    "               self.encoder.weight.norm(p='nuc'), \\\n",
    "               self.decoder.weight.norm(p='nuc'),\n",
    "\n",
    "    def get_reconstruction_loss(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "\n",
    "        recon_loss = torch.sum((x - recon) ** 2) / len(x)\n",
    "        return recon_loss\n",
    "\n",
    "    def get_reg_weights_np(self):\n",
    "        if self.weight_reg_type is None:\n",
    "            return np.zeros(self.hidden_dim)\n",
    "        return np.array(self.l2_reg_list)\n",
    "\n",
    "    def _get_reg_loss(self):\n",
    "        # Standard L2 regularization, applied to W2W1 (product loss)\n",
    "        if self.weight_reg_type == 'uniform_product':\n",
    "            return self.l2_reg_scalar * (torch.norm(torch.matmul(self.decoder.weight, self.encoder.weight)) ** 2)\n",
    "\n",
    "        # Standard L2 regularization for encoder and decoder separately (sum loss)\n",
    "        elif self.weight_reg_type == 'uniform_sum':\n",
    "            # regularize both encoder and decoder\n",
    "            return self.l2_reg_scalar * (torch.norm(self.encoder.weight) ** 2 + torch.norm(self.decoder.weight) ** 2)\n",
    "\n",
    "        # non-uniform sum\n",
    "        elif self.weight_reg_type == 'non_uniform_sum':\n",
    "            return torch.norm(self.diag_weights @ self.encoder.weight) ** 2 \\\n",
    "                   + torch.norm(self.decoder.weight @ self.diag_weights) ** 2\n",
    "\n",
    "        # Do not apply regularization\n",
    "        elif self.weight_reg_type is None:\n",
    "            return 0.0\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"weight_reg_type should be one of (uniform_product, uniform_sum, non_uniform_sum, None)\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e563e10",
   "metadata": {},
   "source": [
    "#### DEFINE MODEL TRAINING FUNCTION train_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c72ca921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE MODEL TRAINING FUNCTION train_models ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_models(data_loader, train_itr, metrics_dict, model_configs, eval_metrics_list=None):\n",
    "    # Initialize model\n",
    "\n",
    "    for train_i in range(train_itr):\n",
    "        for x in data_loader:\n",
    "            x_cuda = x.to(device)\n",
    "\n",
    "            # ---- Optimize ----\n",
    "            losses = {}\n",
    "\n",
    "            model = model_config.get_model()\n",
    "            optimizer = model_config.get_optimizer()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(x_cuda)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # ROTATION\n",
    "            y = model.encoder.weight @ x_cuda.T\n",
    "            yy_t_norm = y @ y.T / float(len(x))\n",
    "            yy_t_upper = yy_t_norm - yy_t_norm.tril()\n",
    "            gamma = 0.5 * (yy_t_upper - yy_t_upper.T)\n",
    "            model.encoder.weight.grad -= gamma @ model.encoder.weight\n",
    "            model.decoder.weight.grad -= model.decoder.weight @ gamma.T\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[model_config.name] = loss.item()\n",
    "\n",
    "        # ---- Log statistics ----\n",
    "        if train_i == 0 or (train_i + 1) % 10 == 0:\n",
    "            print(\"\".join([\"Iteration = {}, Losses: \".format(train_i + 1)]\n",
    "                          + [\"{} = {} \".format(key, val) for key, val in losses.items()]))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd6378",
   "metadata": {},
   "source": [
    "### TRAIN A MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d828ce",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c6a66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GET DATA ####\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed=1234\n",
    "# set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "input_dim = 1000\n",
    "hidden_dim = 400\n",
    "\n",
    "n_data = 5000\n",
    "batch_size = n_data\n",
    "\n",
    "max_sv = float(input_dim) * 0.1\n",
    "min_sv = 1.0\n",
    "sigma = 0.5\n",
    "\n",
    "gt_data = DataGeneratorPCA(input_dim, hidden_dim, min_sv=min_sv, max_sv=max_sv, total=n_data)\n",
    "data = DataGeneratorPCA(input_dim, hidden_dim, load_data=gt_data.x_sample)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c51ed",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90e48371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'rotation', 'model_type': 'rotation', 'model_class': <class '__main__.LinearAE'>, 'extra_model_args': {'weight_reg_type': None}, 'input_dim': 1000, 'hidden_dim': 400, 'init_scale': 0.0001, 'optim_class': <class 'torch.optim.sgd.SGD'>, 'extra_optim_args': {'momentum': 0.9, 'nesterov': True}, 'lr': 0.0001, 'train_itr': 50000, 'seed': 1234} \n",
      "\n",
      "LinearAE(\n",
      "  (encoder): Linear(in_features=1000, out_features=400, bias=False)\n",
      "  (decoder): Linear(in_features=400, out_features=1000, bias=False)\n",
      ") \n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#### Define the model ####\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#### DEFINE MODEL #####\n",
    "model_dict = dict(\n",
    "    model_name='rotation',\n",
    "    model_type='rotation',\n",
    "    model_class=LinearAE,\n",
    "    extra_model_args = {\"weight_reg_type\": None},\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    init_scale=0.0001,\n",
    "    optim_class=torch.optim.SGD,\n",
    "    extra_optim_args={'momentum': 0.9, 'nesterov': True},\n",
    "    lr=0.0001,\n",
    "#     optim_class=torch.optim.Adam,\n",
    "#     extra_optim_args={},\n",
    "#     lr=0.0003,\n",
    "    train_itr=50000,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# model config contains the model \n",
    "model_config = ModelConfig(\n",
    "        model_name=model_dict['model_name'],\n",
    "        model_type=model_dict['model_type'],\n",
    "        model_class=model_dict['model_class'],\n",
    "        input_dim=model_dict['input_dim'], \n",
    "        hidden_dim=model_dict['hidden_dim'],\n",
    "        init_scale=model_dict['init_scale'],\n",
    "        extra_model_args=model_dict['extra_model_args'],\n",
    "        optim_class=model_dict['optim_class'],\n",
    "        lr=model_dict['lr'],\n",
    "        extra_optim_args=model_dict['extra_optim_args']\n",
    "    )\n",
    "\n",
    "print(model_dict,'\\n')\n",
    "print(model_config.get_model(),'\\n')\n",
    "print(model_config.get_optimizer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b34e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1, Losses: rotation = 50555.9375 \n",
      "Iteration = 10, Losses: rotation = 50555.34765625 \n",
      "Iteration = 20, Losses: rotation = 50550.796875 \n",
      "Iteration = 30, Losses: rotation = 50504.23828125 \n",
      "Iteration = 40, Losses: rotation = 49986.59765625 \n",
      "Iteration = 50, Losses: rotation = 45532.6171875 \n",
      "Iteration = 60, Losses: rotation = 36312.390625 \n",
      "Iteration = 70, Losses: rotation = 28885.900390625 \n",
      "Iteration = 80, Losses: rotation = 23576.6640625 \n",
      "Iteration = 90, Losses: rotation = 19933.216796875 \n",
      "Iteration = 100, Losses: rotation = 17541.251953125 \n",
      "Iteration = 110, Losses: rotation = 16143.638671875 \n",
      "Iteration = 120, Losses: rotation = 15526.8818359375 \n",
      "Iteration = 130, Losses: rotation = 15380.6845703125 \n",
      "Iteration = 140, Losses: rotation = 15314.5166015625 \n",
      "Iteration = 150, Losses: rotation = 15274.9619140625 \n",
      "Iteration = 160, Losses: rotation = 15249.8876953125 \n",
      "Iteration = 170, Losses: rotation = 15230.1982421875 \n",
      "Iteration = 180, Losses: rotation = 15214.353515625 \n",
      "Iteration = 190, Losses: rotation = 15201.0625 \n",
      "Iteration = 200, Losses: rotation = 15189.8193359375 \n"
     ]
    }
   ],
   "source": [
    "trained_model = train_models(data_loader=loader, train_itr=model_dict['train_itr'], metrics_dict=None, model_configs=model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9fd38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
