{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85243247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = ('ignore::UserWarning,ignore::ConvergenceWarning,ignore::RuntimeWarning')\n",
    "\n",
    "os.chdir('../..')\n",
    "from pcmf import pcmf_full, path_plot, plot_ordercolor, plot_cluster_assignments, PCMF_predict_clusters\n",
    "from p3ca import cluster_metrics, calculate_scores_nonpath\n",
    "import numpy as np\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0525e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering, KMeans, AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.optimize import linprog, linear_sum_assignment as linear_assignment\n",
    "\n",
    "def confusion_matrix_ordered(pred, true):\n",
    "    def _make_cost_m(cm):\n",
    "        s = np.max(cm)\n",
    "        return (- cm + s)\n",
    "    conf_mat = confusion_matrix(pred,true)\n",
    "    indexes = linear_assignment(_make_cost_m(conf_mat))\n",
    "    js = [e for e in sorted(indexes, key=lambda x: x[0])[1]]\n",
    "    conf_mat_ord = conf_mat[:, js]\n",
    "    return conf_mat_ord\n",
    "\n",
    "# Synthetic data\n",
    "def load_syntheticDataConsensus(n=100000, n_test=10000, p=1000, m=25000, m_test=2500, num_clusters=4, means=[-1.0, 1.0, -0.4, 0.4], r=1, sigma=0.075, density=0.5, plot=False, randomize=False, pcmf_dir=\"/athena/listonlab/store/amb2022/PCMF/\"):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    os.chdir(pcmf_dir)\n",
    "    from pcmf import generate_cluster_PMD_data\n",
    "    print('Loading synthetic data, run: '+str(r))\n",
    "    seeds = [r,r+1,r+2,r+3]\n",
    "    \n",
    "    # Cluster sizes\n",
    "    ms = [m+m_test,m+m_test,m+m_test,m+m_test]\n",
    "    \n",
    "    # Get clustered CCA data\n",
    "    X_clusters, u_true, v_true, _ = generate_cluster_PMD_data(ms, p, sigma, density, num_clusters, means=means) \n",
    "    # Training set\n",
    "    X_c_train = []\n",
    "    X_in_test = []\n",
    "    for nc in range(num_clusters):\n",
    "        X_c_train.append(X_clusters[nc][0:m,:])\n",
    "        X_in_test.append(X_clusters[nc][m:m+m_test,:])\n",
    "    X_c_train = np.vstack(X_c_train)\n",
    "    true_clusters = np.repeat([0,1,2,3],m)\n",
    "    # Test set\n",
    "    X_in_test = np.vstack(X_in_test)\n",
    "    true_clusters_in_test = np.repeat([0,1,2,3],m_test)\n",
    "    \n",
    "    # Shape of dataset\n",
    "    print('X_c_train: ' + str(X_c_train.shape))\n",
    "    print('Y_train: ' + str(true_clusters.shape))\n",
    "    print('X_c_test:  '  + str(X_in_test.shape))\n",
    "    print('Y_test:  '  + str(true_clusters_in_test.shape))\n",
    "    \n",
    "    # Randomize training setorder\n",
    "    if randomize is True:\n",
    "        print('Randomizing order seed 1234')\n",
    "        idx_perm = np.random.RandomState(seed=1234).permutation(X_c_train.shape[0])\n",
    "        X_in = X_c_train[idx_perm,:]\n",
    "        true_clusters_in = true_clusters[idx_perm]\n",
    "    else:\n",
    "        X_in = X_c_train\n",
    "        true_clusters_in = true_clusters\n",
    "    \n",
    "    return X_in, true_clusters_in, X_in_test, true_clusters_in_test, num_clusters, u_true, v_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5267e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading synthetic data, run: 1\n",
      "X_c_train: (100000, 10)\n",
      "Y_train: (100000,)\n",
      "X_c_test:  (10000, 10)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_in, true_clusters_in, X_in_test, true_clusters_in_test, num_clusters, u_true, v_true = load_syntheticDataConsensus(n=100000, n_test=10000, p=10, m=25000, m_test=2500, plot=False, randomize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4499e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data(X_in,true_clusters_in):\n",
    "    from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(X_in)\n",
    "    x = scalerX.transform(X_in).astype(np.float32)\n",
    "\n",
    "    y = pd.factorize(true_clusters_in)[0].astype(np.int32)\n",
    "    print(y.shape)\n",
    "\n",
    "    print('samples', x.shape, y.shape)\n",
    "    return x, y\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class SyntheticDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x, self.y = load_data(X_in,true_clusters_in)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(np.array(self.x[idx])), torch.from_numpy(\n",
    "            np.array(self.y[idx])), torch.from_numpy(np.array(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e8ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################\n",
    "# Evaluate Critiron\n",
    "#######################################################\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as NMI_score, adjusted_rand_score as ARI_score, rand_score as rand_score\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "#     from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "#     return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "    return sum([w[i, j] for i, j in zip(ind[0],ind[1])])*1.0/y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2302d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright Â© dawnranger.\n",
    "#\n",
    "# 2018-05-08 10:15 <dawnranger123@gmail.com>\n",
    "#\n",
    "# Distributed under terms of the MIT license.\n",
    "# https://github.com/dawnranger/IDEC-pytorch\n",
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
    "                 n_input, n_z):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.enc_1 = Linear(n_input, n_enc_1)\n",
    "        self.enc_2 = Linear(n_enc_1, n_enc_2)\n",
    "        self.enc_3 = Linear(n_enc_2, n_enc_3)\n",
    "\n",
    "        self.z_layer = Linear(n_enc_3, n_z)\n",
    "\n",
    "        # decoder\n",
    "        self.dec_1 = Linear(n_z, n_dec_1)\n",
    "        self.dec_2 = Linear(n_dec_1, n_dec_2)\n",
    "        self.dec_3 = Linear(n_dec_2, n_dec_3)\n",
    "\n",
    "        self.x_bar_layer = Linear(n_dec_3, n_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        enc_h1 = F.relu(self.enc_1(x))\n",
    "        enc_h2 = F.relu(self.enc_2(enc_h1))\n",
    "        enc_h3 = F.relu(self.enc_3(enc_h2))\n",
    "\n",
    "        z = self.z_layer(enc_h3)\n",
    "\n",
    "        # decoder\n",
    "        dec_h1 = F.relu(self.dec_1(z))\n",
    "        dec_h2 = F.relu(self.dec_2(dec_h1))\n",
    "        dec_h3 = F.relu(self.dec_3(dec_h2))\n",
    "        x_bar = self.x_bar_layer(dec_h3)\n",
    "\n",
    "        return x_bar, z\n",
    "\n",
    "\n",
    "class IDEC(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_enc_1,\n",
    "                 n_enc_2,\n",
    "                 n_enc_3,\n",
    "                 n_dec_1,\n",
    "                 n_dec_2,\n",
    "                 n_dec_3,\n",
    "                 n_input,\n",
    "                 n_z,\n",
    "                 n_clusters,\n",
    "                 alpha=1,\n",
    "                 pretrain_path='data/ae_mnist.pkl'):\n",
    "        super(IDEC, self).__init__()\n",
    "        self.alpha = 1.0\n",
    "        self.pretrain_path = pretrain_path\n",
    "\n",
    "        self.ae = AE(\n",
    "            n_enc_1=n_enc_1,\n",
    "            n_enc_2=n_enc_2,\n",
    "            n_enc_3=n_enc_3,\n",
    "            n_dec_1=n_dec_1,\n",
    "            n_dec_2=n_dec_2,\n",
    "            n_dec_3=n_dec_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "        # cluster layer\n",
    "        self.cluster_layer = Parameter(torch.Tensor(n_clusters, n_z))\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "    def pretrain(self, path='',pretrain_epochs=200):\n",
    "        if path == '':\n",
    "            pretrain_ae(self.ae,pretrain_epochs)\n",
    "        # load pretrain weights\n",
    "        self.ae.load_state_dict(torch.load(self.pretrain_path))\n",
    "        print('load pretrained ae from', path)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_bar, z = self.ae(x)\n",
    "        # cluster\n",
    "        q = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.alpha)\n",
    "        q = q.pow((self.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "        return x_bar, q\n",
    "\n",
    "\n",
    "def target_distribution(q):\n",
    "    weight = q**2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "\n",
    "def pretrain_ae(model,pretrain_epochs=200):\n",
    "    '''\n",
    "    pretrain autoencoder\n",
    "    '''\n",
    "    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    print(model)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    for epoch in range(pretrain_epochs):\n",
    "        total_loss = 0.\n",
    "        for batch_idx, (x, _, _) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_bar, z = model(x)\n",
    "            loss = F.mse_loss(x_bar, x)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch {} loss={:.4f}\".format(epoch,\n",
    "                                            total_loss / (batch_idx + 1)))\n",
    "        torch.save(model.state_dict(), args.pretrain_path)\n",
    "    print(\"model saved to {}.\".format(args.pretrain_path))\n",
    "\n",
    "\n",
    "def train_idec(model,pretrain_epochs=200,train_epochs=100):\n",
    "\n",
    "#     model = IDEC(\n",
    "#         n_enc_1=500,\n",
    "#         n_enc_2=500,\n",
    "#         n_enc_3=1000,\n",
    "#         n_dec_1=1000,\n",
    "#         n_dec_2=500,\n",
    "#         n_dec_3=500,\n",
    "#         n_input=args.n_input,\n",
    "#         n_z=args.n_z,\n",
    "#         n_clusters=args.n_clusters,\n",
    "#         alpha=1.0,\n",
    "#         pretrain_path=args.pretrain_path).to(device)\n",
    "\n",
    "    #  model.pretrain('data/ae_mnist.pkl')\n",
    "    model.pretrain(pretrain_epochs=pretrain_epochs)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # cluster parameter initiate\n",
    "    data = dataset.x\n",
    "    y = dataset.y\n",
    "    data = torch.Tensor(data).to(device)\n",
    "    x_bar, hidden = model.ae(data)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "    y_pred = kmeans.fit_predict(hidden.data.cpu().numpy())\n",
    "    nmi_k = nmi_score(y_pred, y)\n",
    "    print(\"nmi score={:.4f}\".format(nmi_k))\n",
    "\n",
    "    hidden = None\n",
    "    x_bar = None\n",
    "\n",
    "    y_pred_last = y_pred\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(train_epochs):\n",
    "\n",
    "        if epoch % args.update_interval == 0:\n",
    "\n",
    "            _, tmp_q = model(data)\n",
    "\n",
    "            # update target distribution p\n",
    "            tmp_q = tmp_q.data\n",
    "            p = target_distribution(tmp_q)\n",
    "\n",
    "            # evaluate clustering performance\n",
    "            y_pred = tmp_q.cpu().numpy().argmax(1)\n",
    "            delta_label = np.sum(y_pred != y_pred_last).astype(\n",
    "                np.float32) / y_pred.shape[0]\n",
    "            y_pred_last = y_pred\n",
    "\n",
    "            acc = cluster_acc(y, y_pred)\n",
    "            nmi = nmi_score(y, y_pred)\n",
    "            ari = ari_score(y, y_pred)\n",
    "            print('Iter {}'.format(epoch), ':Acc {:.4f}'.format(acc),\n",
    "                  ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
    "\n",
    "            if epoch > 0 and delta_label < args.tol:\n",
    "                print('delta_label {:.4f}'.format(delta_label), '< tol',\n",
    "                      args.tol)\n",
    "                print('Reached tolerance threshold. Stopping training.')\n",
    "                break\n",
    "        for batch_idx, (x, _, idx) in enumerate(train_loader):\n",
    "\n",
    "            x = x.to(device)\n",
    "            idx = idx.to(device)\n",
    "\n",
    "            x_bar, q = model(x)\n",
    "\n",
    "            reconstr_loss = F.mse_loss(x_bar, x)\n",
    "            kl_loss = F.kl_div(q.log(), p[idx])\n",
    "            loss = args.gamma * kl_loss + reconstr_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768f149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n",
      "samples (100000, 10) (100000,)\n",
      "namespace(batch_size=15, dataset='Synthetic', gamma=0.1, lr=0.001, n_clusters=4, n_input=10, n_z=10, pretrain_path='data/ae_Synthetic.pkl', tol=0.001, update_interval=1)\n",
      "AE(\n",
      "  (enc_1): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (enc_3): Linear(in_features=500, out_features=1000, bias=True)\n",
      "  (z_layer): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  (dec_1): Linear(in_features=10, out_features=1000, bias=True)\n",
      "  (dec_2): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (x_bar_layer): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n",
      "epoch 0 loss=0.0117\n",
      "epoch 1 loss=0.0065\n",
      "epoch 2 loss=0.0059\n",
      "epoch 3 loss=0.0054\n",
      "epoch 4 loss=0.0052\n",
      "epoch 5 loss=0.0046\n",
      "epoch 6 loss=0.0042\n",
      "epoch 7 loss=0.0035\n",
      "epoch 8 loss=0.0034\n",
      "epoch 9 loss=0.0030\n",
      "epoch 10 loss=0.0029\n",
      "epoch 11 loss=0.0028\n",
      "epoch 12 loss=0.0028\n",
      "epoch 13 loss=0.0028\n",
      "epoch 14 loss=0.0025\n",
      "epoch 15 loss=0.0024\n",
      "epoch 16 loss=0.0024\n",
      "epoch 17 loss=0.0023\n",
      "epoch 18 loss=0.0022\n",
      "epoch 19 loss=0.0020\n",
      "epoch 20 loss=0.0018\n",
      "epoch 21 loss=0.0016\n",
      "epoch 22 loss=0.0016\n",
      "epoch 23 loss=0.0016\n",
      "epoch 24 loss=0.0017\n",
      "epoch 25 loss=0.0016\n",
      "epoch 26 loss=0.0017\n",
      "epoch 27 loss=0.0016\n",
      "epoch 28 loss=0.0016\n",
      "epoch 29 loss=0.0016\n",
      "epoch 30 loss=0.0016\n",
      "epoch 31 loss=0.0015\n",
      "epoch 32 loss=0.0014\n",
      "epoch 33 loss=0.0013\n",
      "epoch 34 loss=0.0013\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/athena/listonlab/store/amb2022/PCMF'\n",
    "\n",
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    ")\n",
    "batch_size_options = [15, 30]\n",
    "pretrain_epochs_options = [100, 1000]\n",
    "train_epochs_options = [100, 1000]\n",
    "\n",
    "accuracies = []\n",
    "idx = 0\n",
    "for batch_size in batch_size_options:\n",
    "    for pretrain_epochs in pretrain_epochs_options:\n",
    "        for train_epochs in train_epochs_options:\n",
    "            args.n_z = 10\n",
    "            args.lr = 0.001\n",
    "            args.n_clusters = 4\n",
    "            args.dataset = 'Synthetic'\n",
    "            args.gamma = 0.1\n",
    "            args.update_interval = 1\n",
    "            args.tol = 0.001\n",
    "            args.batch_size = batch_size\n",
    "\n",
    "            os.chdir(data_dir)\n",
    "            if args.dataset == 'Synthetic':\n",
    "                args.pretrain_path = 'data/ae_Synthetic.pkl'\n",
    "                dataset = SyntheticDataset()\n",
    "                args.n_input = dataset.x.shape[1]\n",
    "                args.n_clusters = len(np.unique(dataset.y))\n",
    "            device = 'cpu'\n",
    "            print(args)\n",
    "\n",
    "            tic = time.time()\n",
    "            Synthetic_model = IDEC(\n",
    "                    n_enc_1=500,\n",
    "                    n_enc_2=500,\n",
    "                    n_enc_3=1000,\n",
    "                    n_dec_1=1000,\n",
    "                    n_dec_2=500,\n",
    "                    n_dec_3=500,\n",
    "                    n_input=args.n_input,\n",
    "                    n_z=args.n_z,\n",
    "                    n_clusters=args.n_clusters,\n",
    "                    alpha=1.0,\n",
    "                    pretrain_path=args.pretrain_path).to(device)\n",
    "\n",
    "            train_idec(Synthetic_model,pretrain_epochs=pretrain_epochs,train_epochs=train_epochs)\n",
    "\n",
    "            toc = time.time() - tic\n",
    "            print('Time elapsed:',toc)\n",
    "\n",
    "            data = dataset.x\n",
    "            y = dataset.y\n",
    "            data = torch.Tensor(data).to(device)\n",
    "            x_bar, hidden = Synthetic_model.ae(data)\n",
    "\n",
    "            # evaluate clustering performance\n",
    "            _, tmp_q = Synthetic_model(data)\n",
    "\n",
    "            # update target distribution p\n",
    "            tmp_q = tmp_q.data\n",
    "            p = target_distribution(tmp_q)\n",
    "\n",
    "            y_pred = tmp_q.cpu().numpy().argmax(1)\n",
    "\n",
    "            acc = cluster_acc(y, y_pred)\n",
    "            nmi = nmi_score(y, y_pred)\n",
    "            ari = ari_score(y, y_pred)\n",
    "            print('Acc {:.4f}'.format(acc),\n",
    "                  ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            conf_mat_ord = confusion_matrix_ordered(y_pred, y)\n",
    "            acc = np.sum(np.diag(conf_mat_ord))/np.sum(conf_mat_ord)\n",
    "            print('IDX:',idx, 'Accuracy:', acc, 'Batch size:',batch_size, 'pretrain_epochs:',pretrain_epochs, 'train_epochs:',train_epochs)\n",
    "            idx = idx+1\n",
    "\n",
    "            accuracies.append([idx,acc, batch_size, pretrain_epochs, pretrain_epochs])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e678b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581931d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabb0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeca096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
