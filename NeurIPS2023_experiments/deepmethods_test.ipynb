{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226452ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 13:05:30.582907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from pcmf_neurips2022 import pcmf_PALS, pcmf_ADMM, pcmf_ADMM_consensus, pcmf_CCA, two_cluster_data, path_plot\n",
    "\n",
    "# from palmerpenguins import load_penguins\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = ('ignore::UserWarning,ignore::ConvergenceWarning,ignore::RuntimeWarning')\n",
    "\n",
    "os.chdir('/Users/amandabuch/Documents/clusterCCA/PCMF')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pcmf_dataloaders import load_NCI, load_SRBCT, load_mouseorgans, load_gbmBreastLung, load_penguins\n",
    "from pcmf_deepcomparisons import load_data, PenguinsDataset, fit_dec, fit_IDEC_deep_cluster, fit_cardec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2321f2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amandabuch/Documents/clusterCCA/PCMF'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0572e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcmf_deepcomparisons import fit_IDEC_deep_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506bbb80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_in penguins\n",
      "(342, 2) (342, 2)\n",
      "Penguins samples (342, 24) (342,)\n",
      "namespace(n_z=10, lr=0.001, n_clusters=3, dataset='penguins', gamma=0.1, update_interval=1, tol=0.001, batch_size=15, pretrain_path='data/ae_penguins.pkl', n_input=24)\n",
      "training\n",
      "data/ae_penguins.pkl\n",
      "load pretrained ae from data/ae_penguins.pkl\n",
      "nmi score=0.7707\n",
      "Iter 0 :Acc 0.9064 , nmi 0.7707 , ari 0.7749\n",
      "Iter 1 :Acc 0.9035 , nmi 0.7708 , ari 0.7688\n",
      "Iter 2 :Acc 0.8918 , nmi 0.7537 , ari 0.7462\n",
      "Iter 3 :Acc 0.8977 , nmi 0.7602 , ari 0.7575\n",
      "Iter 4 :Acc 0.8801 , nmi 0.7287 , ari 0.7211\n",
      "Iter 5 :Acc 0.9386 , nmi 0.8178 , ari 0.8446\n",
      "Iter 6 :Acc 0.7924 , nmi 0.6259 , ari 0.5730\n",
      "Iter 7 :Acc 0.9327 , nmi 0.7847 , ari 0.8201\n",
      "Iter 8 :Acc 0.9269 , nmi 0.7988 , ari 0.8182\n",
      "Iter 9 :Acc 0.8977 , nmi 0.7121 , ari 0.7306\n",
      "Time elapsed: 3.5412352085113525\n",
      "Acc 0.9064 , nmi 0.7675 , ari 0.7751\n",
      "IDX: 0 Accuracy: 0.9064327485380117 Batch size: 15 pretrain_epochs: 10 train_epochs: 10\n",
      "Total time elapsed: 3.5412352085113525\n",
      "Penguins: IDEC Accuracy:  90.64 Time elapsed 3.572004795074463\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "accuracies, toc, acc_str = fit_IDEC_deep_cluster(dataset_in='penguins', data_dir='/Users/amandabuch/Documents/clusterCCA/PCMF', batch_size_options=[15], pretrain_epochs_options=[10], train_epochs_options=[10])\n",
    "toc = time.time() - tic\n",
    "accuracies_pd = pd.DataFrame(accuracies, columns = ['idx','Accuracy', 'BatchSize','pretrain_epochs', 'train_epochs'])\n",
    "acc = np.round(accuracies_pd['Accuracy'].max()*100,2)\n",
    "print('Penguins:', 'IDEC','Accuracy: ', str(acc), 'Time elapsed', toc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac38570d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit_cardec() missing 2 required positional arguments: 'X_in' and 'true_clusters_in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m accuracies, toc, acc_str \u001b[38;5;241m=\u001b[39m \u001b[43mfit_cardec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/athena/listonlab/store/amb2022/PCMF/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPenguins\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m      4\u001b[0m accuracies_pd \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(accuracies, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchSize\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetune_iters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayerwise_pretrain_iters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_iter_max\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: fit_cardec() missing 2 required positional arguments: 'X_in' and 'true_clusters_in'"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "accuracies, toc, acc_str = fit_cardec(data_dir='/athena/listonlab/store/amb2022/PCMF/',dataName=\"Penguins\")\n",
    "toc = time.time() - tic\n",
    "accuracies_pd = pd.DataFrame(accuracies, columns = ['idx','Accuracy', 'BatchSize','finetune_iters', 'layerwise_pretrain_iters', 'cluster_iter_max'])\n",
    "acc = accuracies_pd['Accuracy'].max()\n",
    "print('Penguins:', 'CarDEC','Accuracy: ', str(acc), 'Time elapsed', toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load('/Users/amandabuch/Documents/clusterCCA/revision1/clusterCCA/data/SRBCT.npz', allow_pickle=True)\n",
    "list(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5d770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pcmf_deepcomparisons import PenguinsDataset, pretrain_ae\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from types import SimpleNamespace\n",
    "# args = SimpleNamespace()\n",
    "# args.batch_size=10\n",
    "\n",
    "\n",
    "# DataLoader(PenguinsDataset(), batch_size=10, shuffle=True)\n",
    "# pretrain_ae('',PenguinsDataset(), args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pcmf_deepcomparisons import IDEC\n",
    "\n",
    "# data_dir='/Users/amandabuch/Documents/clusterCCA/PCMF'\n",
    "# dataset_in = 'penguins'\n",
    "# batch_size_options=[15]\n",
    "# pretrain_epochs_options=[10]\n",
    "# train_epochs_options=[10]\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "# from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.parameter import Parameter\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.nn import Linear\n",
    "# tic0 = time.time()\n",
    "# from types import SimpleNamespace\n",
    "# args = SimpleNamespace(\n",
    "# )\n",
    "# # batch_size_options = [15, 30]\n",
    "# # pretrain_epochs_options = [100, 1000]\n",
    "# # train_epochs_options = [100, 1000]\n",
    "# print('dataset_in', dataset_in)\n",
    "# accuracies = []\n",
    "# idx = 0\n",
    "# for batch_size in batch_size_options:\n",
    "#     for pretrain_epochs in pretrain_epochs_options:\n",
    "#         for train_epochs in train_epochs_options:\n",
    "#             args.n_z = 10\n",
    "#             args.lr = 0.001\n",
    "#             args.n_clusters = 6\n",
    "#             args.dataset = dataset_in #\n",
    "#             args.gamma = 0.1\n",
    "#             args.update_interval = 1\n",
    "#             args.tol = 0.001\n",
    "#             args.batch_size = batch_size\n",
    "\n",
    "#             os.chdir(data_dir)\n",
    "\n",
    "#             if args.dataset == 'penguins':\n",
    "#                 from pcmf_dataloaders import load_penguins\n",
    "#                 args.pretrain_path = 'data/ae_penguins.pkl'\n",
    "#                 dataset = PenguinsDataset()\n",
    "#                 args.n_input = dataset.x.shape[1]\n",
    "#                 args.n_clusters = len(np.unique(dataset.y))\n",
    "\n",
    "#             device = 'cpu'\n",
    "#             print(args)\n",
    "\n",
    "#             tic = time.time()\n",
    "#             TRAIN_model = IDEC(\n",
    "#                     n_enc_1=500,\n",
    "#                     n_enc_2=500,\n",
    "#                     n_enc_3=1000,\n",
    "#                     n_dec_1=1000,\n",
    "#                     n_dec_2=500,\n",
    "#                     n_dec_3=500,\n",
    "#                     n_input=args.n_input,\n",
    "#                     n_z=args.n_z,\n",
    "#                     n_clusters=args.n_clusters,\n",
    "#                     alpha=1.0,\n",
    "#                     pretrain_path=args.pretrain_path).to(device)\n",
    "\n",
    "#             train_idec(TRAIN_model, dataset, args, pretrain_epochs=pretrain_epochs, train_epochs=train_epochs)\n",
    "\n",
    "#             toc = time.time() - tic\n",
    "#             print('Time elapsed:',toc)\n",
    "\n",
    "#             data = dataset.x\n",
    "#             y = dataset.y\n",
    "#             data = torch.Tensor(data).to(device)\n",
    "#             x_bar, hidden = TRAIN_model.ae(data)\n",
    "\n",
    "#             # evaluate clustering performance\n",
    "#             _, tmp_q = TRAIN_model(data)\n",
    "\n",
    "#             # update target distribution p\n",
    "#             tmp_q = tmp_q.data\n",
    "#             p = target_distribution(tmp_q)\n",
    "\n",
    "#             y_pred = tmp_q.cpu().numpy().argmax(1)\n",
    "\n",
    "#             acc = cluster_acc(y, y_pred)\n",
    "#             nmi = nmi_score(y, y_pred)\n",
    "#             ari = ari_score(y, y_pred)\n",
    "#             print('Acc {:.4f}'.format(acc),\n",
    "#                   ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
    "\n",
    "#             # Calculate accuracy\n",
    "#             conf_mat_ord = confusion_matrix_ordered(y_pred, y)\n",
    "#             acc = np.sum(np.diag(conf_mat_ord))/np.sum(conf_mat_ord)\n",
    "#             print('IDX:',idx, 'Accuracy:', acc, 'Batch size:',batch_size, 'pretrain_epochs:',pretrain_epochs, 'train_epochs:',train_epochs)\n",
    "#             idx = idx+1\n",
    "\n",
    "#             accuracies.append([idx, acc, batch_size, pretrain_epochs, pretrain_epochs])\n",
    "\n",
    "#     toc2 = time.time() - tic0\n",
    "#     print('Total time elapsed:',toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d05b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342, 2) (342, 2)\n",
      "Penguins samples (342, 24) (342,)\n",
      "(342,)\n",
      "samples (342, 24) (342,)\n",
      "WARNING: adata.X seems to be already log-transformed.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m XYall, true_clusters, Xall, Yall \u001b[38;5;241m=\u001b[39m load_penguins()\n\u001b[0;32m----> 3\u001b[0m accuracies, toc, acc_str \u001b[38;5;241m=\u001b[39m \u001b[43mfit_cardec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXYall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/amandabuch/Documents/clusterCCA/PCMF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPenguins\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m      5\u001b[0m accuracies_pd \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(accuracies, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchSize\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetune_iters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayerwise_pretrain_iters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_iter_max\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/clusterCCA/PCMF/pcmf_deepcomparisons.py:2998\u001b[0m, in \u001b[0;36mfit_cardec\u001b[0;34m(X_in, true_clusters_in, data_dir, dataName, n_top_genes_options, n_neighbors_options)\u001b[0m\n\u001b[1;32m   2996\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mhighly_variable_genes(adata) \u001b[38;5;66;03m# ,n_top_genes=n_top_genes\u001b[39;00m\n\u001b[1;32m   2997\u001b[0m \u001b[38;5;66;03m# Initialize and pretrain weights\u001b[39;00m\n\u001b[0;32m-> 2998\u001b[0m CarDEC \u001b[38;5;241m=\u001b[39m \u001b[43mCarDEC_API\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataName\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_Weights_NN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_NG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLVG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_high_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[1;32m   3000\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad_reconstruction\n",
      "File \u001b[0;32m~/Documents/clusterCCA/PCMF/pcmf_deepcomparisons.py:2581\u001b[0m, in \u001b[0;36mCarDEC_API.__init__\u001b[0;34m(self, adata, preprocess, weights_dir, batch_key, n_high_var, LVG, normalize_samples, log_normalize, normalize_features)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_args \u001b[38;5;241m=\u001b[39m (batch_key, n_high_var, LVG, normalize_samples, log_normalize, normalize_features)\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_scanpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariance Type\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m adata\u001b[38;5;241m.\u001b[39mvar\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/Documents/clusterCCA/PCMF/pcmf_deepcomparisons.py:736\u001b[0m, in \u001b[0;36mnormalize_scanpy\u001b[0;34m(adata, batch_key, n_high_var, LVG, normalize_samples, log_normalize, normalize_features)\u001b[0m\n\u001b[1;32m    733\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mlog1p(adata)\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_high_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhighly_variable_genes\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_disp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_high_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m     hvg \u001b[38;5;241m=\u001b[39m adata\u001b[38;5;241m.\u001b[39mvar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighly_variable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m log_normalize:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamba/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440\u001b[0m, in \u001b[0;36mhighly_variable_genes\u001b[0;34m(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _highly_variable_genes_seurat_v3(\n\u001b[1;32m    429\u001b[0m         adata,\n\u001b[1;32m    430\u001b[0m         layer\u001b[38;5;241m=\u001b[39mlayer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m    437\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43m_highly_variable_genes_single_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_disp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_disp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_disp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_disp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_top_genes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m     sanitize_anndata(adata)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamba/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:274\u001b[0m, in \u001b[0;36m_highly_variable_genes_single_batch\u001b[0;34m(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor)\u001b[0m\n\u001b[1;32m    269\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`n_top_genes` > number of normalized dispersions, returning all genes with normalized dispersions.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    273\u001b[0m     n_top_genes \u001b[38;5;241m=\u001b[39m dispersion_norm\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m--> 274\u001b[0m disp_cut_off \u001b[38;5;241m=\u001b[39m \u001b[43mdispersion_norm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_top_genes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    275\u001b[0m gene_subset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdispersions_norm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m disp_cut_off\n\u001b[1;32m    276\u001b[0m logg\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_top_genes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m top genes correspond to a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized dispersion cutoff of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisp_cut_off\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    279\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "XYall, true_clusters, Xall, Yall = load_penguins()\n",
    "accuracies, toc, acc_str = fit_cardec(XYall, true_clusters, data_dir='/Users/amandabuch/Documents/clusterCCA/PCMF',dataName=\"Penguins\")\n",
    "toc = time.time() - tic\n",
    "accuracies_pd = pd.DataFrame(accuracies, columns = ['idx','Accuracy', 'BatchSize','finetune_iters', 'layerwise_pretrain_iters', 'cluster_iter_max'])\n",
    "acc = accuracies_pd['Accuracy'].max()\n",
    "print('Penguins:', 'CarDEC','Accuracy: ', str(acc), 'Time elapsed', toc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118a59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_idec(model,dataset,args,pretrain_epochs=200,train_epochs=100):\n",
    "# #     model = IDEC(\n",
    "# #         n_enc_1=500,\n",
    "# #         n_enc_2=500,\n",
    "# #         n_enc_3=1000,\n",
    "# #         n_dec_1=1000,\n",
    "# #         n_dec_2=500,\n",
    "# #         n_dec_3=500,\n",
    "# #         n_input=args.n_input,\n",
    "# #         n_z=args.n_z,\n",
    "# #         n_clusters=args.n_clusters,\n",
    "# #         alpha=1.0,\n",
    "# #         pretrain_path=args.pretrain_path).to(device)\n",
    "\n",
    "#     #  model.pretrain('data/ae_mnist.pkl')\n",
    "#     print('ok')\n",
    "#     print(dataset)\n",
    "# #     pretrain_ae('',PenguinsDataset(), args)\n",
    "#     model.pretrain(dataset, args, pretrain_epochs=pretrain_epochs)\n",
    "\n",
    "#     train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "#     optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "#     # cluster parameter initiate\n",
    "#     data = dataset.x\n",
    "#     y = dataset.y\n",
    "#     data = torch.Tensor(data).to(device)\n",
    "#     x_bar, hidden = model.ae(data)\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "#     y_pred = kmeans.fit_predict(hidden.data.cpu().numpy())\n",
    "#     nmi_k = nmi_score(y_pred, y)\n",
    "#     print(\"nmi score={:.4f}\".format(nmi_k))\n",
    "\n",
    "#     hidden = None\n",
    "#     x_bar = None\n",
    "\n",
    "#     y_pred_last = y_pred\n",
    "#     model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "\n",
    "#     model.train()\n",
    "#     for epoch in range(train_epochs):\n",
    "\n",
    "#         if epoch % args.update_interval == 0:\n",
    "\n",
    "#             _, tmp_q = model(data)\n",
    "\n",
    "#             # update target distribution p\n",
    "#             tmp_q = tmp_q.data\n",
    "#             p = target_distribution(tmp_q)\n",
    "\n",
    "#             # evaluate clustering performance\n",
    "#             y_pred = tmp_q.cpu().numpy().argmax(1)\n",
    "#             delta_label = np.sum(y_pred != y_pred_last).astype(\n",
    "#                 np.float32) / y_pred.shape[0]\n",
    "#             y_pred_last = y_pred\n",
    "\n",
    "#             acc = cluster_acc(y, y_pred)\n",
    "#             nmi = nmi_score(y, y_pred)\n",
    "#             ari = ari_score(y, y_pred)\n",
    "#             print('Iter {}'.format(epoch), ':Acc {:.4f}'.format(acc),\n",
    "#                   ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
    "\n",
    "#             if epoch > 0 and delta_label < args.tol:\n",
    "#                 print('delta_label {:.4f}'.format(delta_label), '< tol',\n",
    "#                       args.tol)\n",
    "#                 print('Reached tolerance threshold. Stopping training.')\n",
    "#                 break\n",
    "#         for batch_idx, (x, _, idx) in enumerate(train_loader):\n",
    "\n",
    "#             x = x.to(device)\n",
    "#             idx = idx.to(device)\n",
    "\n",
    "#             x_bar, q = model(x)\n",
    "\n",
    "#             reconstr_loss = F.mse_loss(x_bar, x)\n",
    "#             kl_loss = F.kl_div(q.log(), p[idx])\n",
    "#             loss = args.gamma * kl_loss + reconstr_loss\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499eeb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8db73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd582186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226607b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb34159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "XYall, true_clusters, Xall, Yall = load_penguins()\n",
    "tic = time.time()\n",
    "accuracies, toc, acc_str = fit_dec(XYall, true_clusters, batch_size_options=[15], finetune_iters_options=[10], layerwise_pretrain_iters_options=[10], cluster_iter_max_options=[10])\n",
    "toc = time.time() - tic\n",
    "accuracies_pd = pd.DataFrame(accuracies, columns = ['idx','Accuracy', 'BatchSize','finetune_iters', 'layerwise_pretrain_iters', 'cluster_iter_max'])\n",
    "acc = np.round(accuracies_pd['Accuracy'].max()*100,2)\n",
    "print('Penguins:', 'DEC','Accuracy: ', str(acc), 'Time elapsed', toc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
